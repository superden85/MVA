{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlAfI8mCWAf3"
      },
      "source": [
        "<center><h2>ALTeGraD 2022<br>Lab Session 2: Transfer learning for NLP</h2> 27 / 10 / 2022<br> M. Kamal Eddine, H. Abdine<br><br>\n",
        "\n",
        "\n",
        "<b>Student name:</b> [Denis DUVAL]\n",
        "\n",
        "</center>\n",
        "\n",
        "<br><br>\n",
        "In this lab we will:\n",
        "* Implement and pretrain a language model with transformer architecture.\n",
        "* Use the pretrained model (transfer learning) to perform a sentiment analysis task which consists of classifying some books reviews into positive and negative ones.\n",
        "* Compare the performance of the pretrained model to a model trained from scratch.\n",
        " <br>\n",
        " \n",
        "<b>The deadline for this lab is November 14, 2022 11:59 PM.</b> More details about the submission and the architecture for this lab can be found in the handout PDF."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IqukuIe0Rb_c"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FF6fjkqgN39"
      },
      "source": [
        "### The Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "p0cj9WkSFQwl"
      },
      "outputs": [],
      "source": [
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, ntokens, nhead, nhid, nlayers, dropout=0.5):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        '''\n",
        "        ntokens: the size of vocabulary\n",
        "        nhid: the hidden dimension of the model.\n",
        "        We assume that embedding_dim = nhid\n",
        "        nlayers: the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
        "        nhead: the number of heads in the multiheadattention models\n",
        "        dropout: the dropout value\n",
        "         '''\n",
        "        self.model_type = \"Transformer\"\n",
        "        self.encoder = nn.Embedding(ntokens,\n",
        "                                    nhid,\n",
        "                                    padding_idx = 1) # fill me, nhid = the dim_embed\n",
        "        self.pos_encoder = PositionalEncoding(nhid, \n",
        "                                              dropout=dropout,) #fill me, the PositionalEncoding class is implemented in the next cell\n",
        "        encoder_layers = nn.TransformerEncoderLayer(nhid, \n",
        "                                                    nhead,\n",
        "                                                    nhid,\n",
        "                                                    dropout=dropout) #fill me we assume nhid = d_model = dim_feedforward\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, \n",
        "                                                         nlayers) #fill me\n",
        "        self.nhid = nhid\n",
        "        self.init_weights()\n",
        "\n",
        "    def generate_square_subsequent_mask(self, sz):\n",
        "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "        mask = (\n",
        "            mask.float()\n",
        "            .masked_fill(mask == 0, float(\"-inf\"))\n",
        "            .masked_fill(mask == 1, float(0.0))\n",
        "        )\n",
        "        return mask\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, src, src_mask):\n",
        "        src = self.encoder(src) * math.sqrt(self.nhid) \n",
        "        src = self.pos_encoder(src)#fill me\n",
        "        output = self.transformer_encoder(src, src_mask)#fill me\n",
        "        return output\n",
        "\n",
        "\n",
        "class ClassificationHead(nn.Module):\n",
        "    def __init__(self, nhid, nclasses):\n",
        "        super(ClassificationHead, self).__init__()\n",
        "        self.decoder = nn.Linear(nhid,\n",
        "                                 nclasses)#fill me)\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.decoder.bias.data.zero_()\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, src):\n",
        "        output = self.decoder(src)\n",
        "        return output\n",
        "    \n",
        "class Model(nn.Module):\n",
        "    def __init__(self, ntokens, nhead, nhid, nlayers, nclasses, dropout=0.5):\n",
        "        super(Model, self).__init__()\n",
        "        self.base = TransformerModel(ntokens, nhead, nhid, nlayers, dropout=dropout)#fill me\n",
        "        self.classifier = ClassificationHead(nhid, nclasses)#fill me\n",
        "\n",
        "    def forward(self, src, src_mask):\n",
        "        # base model\n",
        "        x = self.base(src, src_mask)#fill me\n",
        "        # classifier model\n",
        "        output = self.classifier(x)#fill me\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "kt2QQohaFZry"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, nhid, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, nhid)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(\n",
        "            torch.arange(0, nhid, 2).float() * (-math.log(10000.0) / nhid)\n",
        "        )\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer(\"pe\", pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[: x.size(0), :]\n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfEYHJx2JW6l"
      },
      "source": [
        "Let's verify if our model works, by applying one inference step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "rhb2gkUhJMR0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64a5e927-963d-487a-a17b-fa6a444ccf5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 6, 100])\n"
          ]
        }
      ],
      "source": [
        "ntokens = 100 # the size of vocabulary\n",
        "nhid = 200  # hidden dimension\n",
        "nlayers = 4  # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
        "nhead = 2  # the number of heads in the multiheadattention models\n",
        "dropout = 0  # the dropout value\n",
        "\n",
        "model = Model(ntokens, nhead, nhid, nlayers, ntokens, dropout).to(device)\n",
        "dummy_input = torch.tensor([[2, 6, 2, 5, 43, 21]]).to(device)\n",
        "src_mask = model.base.generate_square_subsequent_mask(1).to(device)\n",
        "out = model.forward(dummy_input, src_mask)\n",
        "\n",
        "print(out.shape) # is it the right shape?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i74NN897Fcit"
      },
      "source": [
        "## Vocabulary and Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "5qjd26ghWuff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96fcf995-19bd-46d5-b165-fb5083d24c99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-11-14 21:37:22--  https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/dict.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 577587 (564K) [text/plain]\n",
            "Saving to: ‘dict.txt’\n",
            "\n",
            "dict.txt            100%[===================>] 564.05K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2022-11-14 21:37:23 (12.2 MB/s) - ‘dict.txt’ saved [577587/577587]\n",
            "\n",
            "▁d 1\n",
            "es 1\n",
            "▁l 1\n",
            "en 1\n",
            "on 1\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/dict.txt\n",
        "!head -5 dict.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "vFdH_-JeFbGA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf1fe85f-6837-4a05-d3e0-7afbebfd30b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "▁trop\n"
          ]
        }
      ],
      "source": [
        "path_vocab = \"dict.txt\"\n",
        "token2ind = {\"<sos>\": 0, \"<pad>\": 1, \"<eos>\": 2, \"<oov>\": 3} # the 4 first indices are reserved to special tokens\n",
        "with open(path_vocab, \"r\") as f:\n",
        "    for idx, line in enumerate(f):\n",
        "        word = line.split()[0].strip()\n",
        "        token2ind[word] = 4 + idx#fill me\n",
        "\n",
        "ind2token = {v:k for k,v in token2ind.items()} #fill me\n",
        "\n",
        "print(ind2token[1111])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOExGODajN8p"
      },
      "source": [
        "### Data Loader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Y0jN-Ar9i5Q1"
      },
      "outputs": [],
      "source": [
        "import numpy\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "\n",
        "class Dataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        path_documents,\n",
        "        path_labels=None,\n",
        "        token2ind={},\n",
        "        max_len=512,\n",
        "        task=\"language_modeling\",\n",
        "    ):\n",
        "        self.task = task\n",
        "        self.max_len = max_len\n",
        "        self.token2ind = token2ind\n",
        "        self.documents = []\n",
        "        self.labels = []\n",
        "        with open(path_documents, \"r\") as f1:\n",
        "            for line in f1:\n",
        "                self.documents.append(line.strip())\n",
        "        if task == \"classification\":\n",
        "            with open(path_labels, \"r\") as f1:\n",
        "                for line in f1:\n",
        "                    self.labels.append(int(line.strip()))\n",
        "            assert len(self.labels) == len(self.documents)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.documents)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        sequence = self.documents[index].split()\n",
        "        if len(sequence) > self.max_len - 1:\n",
        "            sequence = sequence[: self.max_len - 1]\n",
        "        source_sequence = [self.token2ind[\"<sos>\"]]+[self.token2ind[token] if token in self.token2ind.keys() else self.token2ind[\"<oov>\"] for token in sequence]#fill me (constract the input sequence using token2ind, sequence and special tokens)\n",
        "        if self.task == \"language_modeling\":\n",
        "            target = source_sequence[1:]\n",
        "            target.append(self.token2ind[\"<eos>\"])\n",
        "        elif self.task == \"classification\":\n",
        "            target = [self.labels[index]]\n",
        "        sample = {\n",
        "            \"source_sequence\": torch.tensor(source_sequence),\n",
        "            \"target\": torch.tensor(target),\n",
        "        }\n",
        "        return sample\n",
        "\n",
        "\n",
        "def MyCollator(batch):\n",
        "    source_sequences = pad_sequence(\n",
        "        #we use padding to match the length of the sequences in the same batch\n",
        "        [sample[\"source_sequence\"] for sample in batch], padding_value=token2ind[\"<pad>\"]\n",
        "    )\n",
        "    target = pad_sequence(\n",
        "        [sample[\"target\"] for sample in batch], padding_value=token2ind[\"<pad>\"]\n",
        "    )\n",
        "    return source_sequences, target.reshape(-1)\n",
        "\n",
        "\n",
        "def get_loader(\n",
        "    path_documents,\n",
        "    path_labels=None,\n",
        "    token2ind={},\n",
        "    max_len=512,\n",
        "    batch_size=32,\n",
        "    task=\"language_modeling\",\n",
        "):\n",
        "    dataset = Dataset(\n",
        "        path_documents,\n",
        "        path_labels=path_labels,\n",
        "        token2ind=token2ind,\n",
        "        max_len=512,\n",
        "        task=task,\n",
        "    )\n",
        "    data_loader = DataLoader(\n",
        "        dataset=dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        collate_fn=MyCollator,\n",
        "        pin_memory=True,\n",
        "        drop_last=True,\n",
        "    )\n",
        "    return data_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTns4lHrjUTa"
      },
      "source": [
        "## The Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "4_jwosiLjRsS"
      },
      "outputs": [],
      "source": [
        "def train(\n",
        "    path_data_train,\n",
        "    path_labels_train=None,\n",
        "    path_data_valid=None,\n",
        "    save_interval=-1,\n",
        "    log_interval=5,\n",
        "    task=\"language_modeling\",\n",
        "    batch_size=32,\n",
        "):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    ntokens = len(token2ind)\n",
        "    data_loader = get_loader(\n",
        "        path_data_train,\n",
        "        path_labels_train,\n",
        "        token2ind,\n",
        "        task=task,\n",
        "        batch_size=batch_size,\n",
        "    )\n",
        "    \n",
        "    losses = []\n",
        "    for idx, data in enumerate(data_loader): #step 1\n",
        "        optimizer.zero_grad()\n",
        "        src_mask = model.base.generate_square_subsequent_mask(data[0].size(0)).to(\n",
        "            device\n",
        "        )\n",
        "        input = data[0].to(device)\n",
        "        output = model(input, src_mask) #step 2\n",
        "        if task == 'classification':\n",
        "            #last vector only\n",
        "            output = output[-1]#fill me \n",
        "        output = output.view(-1, output.shape[-1])\n",
        "        target =  data[1]#fill me\n",
        "        target = target.to(device)\n",
        "        loss =  criterion(output, target)#fill me, Cross entropy check next cells\n",
        "        loss.backward()\n",
        "        #fill me step 3\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5) # prevent exploding gradient \n",
        "        optimizer.step()\n",
        "        #fill me step 4\n",
        "\n",
        "        total_loss += loss.item() \n",
        "        if idx % log_interval == 0 and idx > 0:\n",
        "            cur_loss = total_loss / log_interval\n",
        "            print(\n",
        "                \"| epoch {:3d} | {:5d}/{:5d} steps | \"\n",
        "                \"loss {:5.5f} | ppl {:8.3f}\".format(\n",
        "                    epoch, idx, len(data_loader), cur_loss, math.exp(cur_loss),\n",
        "                )\n",
        "            )\n",
        "            losses.append(cur_loss)\n",
        "            total_loss = 0\n",
        "    return losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "pgf6BDB9jUr6"
      },
      "outputs": [],
      "source": [
        "ntokens = len(token2ind)#fill me # the size of vocabulary\n",
        "nhid = 200  # the dimension of the feedforward network model in nn.TransformerEncoder\n",
        "nlayers = 4  # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
        "nhead = 2  # the number of heads in the multiheadattention models\n",
        "dropout = 0  # the dropout value\n",
        "\n",
        "nclasses = 2 # for classification task only\n",
        "\n",
        "model = Model(ntokens, nhead, nhid, nlayers, ntokens, dropout).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "u-OLy4KIkDwf"
      },
      "outputs": [],
      "source": [
        "# optimization paramerters\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=token2ind['<pad>'])\n",
        "lr = 0.0003  # learning rate\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Bwh3n9xZQy4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e58c175-7370-4fd8-8423-6fcab1ec6f6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-11-14 21:37:23--  https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/pretraining_subset.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10146460 (9.7M) [text/plain]\n",
            "Saving to: ‘pretraining_subset.txt’\n",
            "\n",
            "pretraining_subset. 100%[===================>]   9.68M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2022-11-14 21:37:24 (100 MB/s) - ‘pretraining_subset.txt’ saved [10146460/10146460]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/pretraining_subset.txt\n",
        "path_data_train = \"pretraining_subset.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "0m11g4ScjZaR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5670a0c6-3194-4294-d792-f5231f4657c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 |   500/ 3125 steps | loss 7.32723 | ppl 1521.158\n",
            "| epoch   1 |  1000/ 3125 steps | loss 6.49288 | ppl  660.420\n",
            "| epoch   1 |  1500/ 3125 steps | loss 6.21702 | ppl  501.207\n",
            "| epoch   1 |  2000/ 3125 steps | loss 6.04752 | ppl  423.065\n",
            "| epoch   1 |  2500/ 3125 steps | loss 5.90390 | ppl  366.464\n",
            "| epoch   1 |  3000/ 3125 steps | loss 5.82515 | ppl  338.711\n",
            "| epoch   2 |   500/ 3125 steps | loss 5.52879 | ppl  251.838\n",
            "| epoch   2 |  1000/ 3125 steps | loss 5.47237 | ppl  238.023\n",
            "| epoch   2 |  1500/ 3125 steps | loss 5.44153 | ppl  230.795\n",
            "| epoch   2 |  2000/ 3125 steps | loss 5.41875 | ppl  225.596\n",
            "| epoch   2 |  2500/ 3125 steps | loss 5.37818 | ppl  216.627\n",
            "| epoch   2 |  3000/ 3125 steps | loss 5.35925 | ppl  212.566\n"
          ]
        }
      ],
      "source": [
        "#pretraining on a tiny subset\n",
        "log_interval = 500\n",
        "epochs = 2\n",
        "for epoch in range(1, epochs + 1): #5\n",
        "    train(\n",
        "        path_data_train,\n",
        "        save_interval=-1,\n",
        "        task=\"language_modeling\", # fill me\n",
        "        batch_size=16,\n",
        "        log_interval=log_interval,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MeOM1dOvkO4e"
      },
      "source": [
        "## Text Generation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "-BcBC6FSkMH3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48534170-8554-48a6-8a47-bb8541933c60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-11-14 21:39:20--  https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/pretrained_model_4layers.pt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 88093955 (84M) [application/octet-stream]\n",
            "Saving to: ‘pretrained_model_4layers.pt’\n",
            "\n",
            "pretrained_model_4l 100%[===================>]  84.01M   215MB/s    in 0.4s    \n",
            "\n",
            "2022-11-14 21:39:22 (215 MB/s) - ‘pretrained_model_4layers.pt’ saved [88093955/88093955]\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/pretrained_model_4layers.pt\n",
        "\n",
        "model = Model(ntokens, nhead, nhid, nlayers, ntokens).to(device) \n",
        "\n",
        "#load the checkpoint\n",
        "checkpoint = torch.load('pretrained_model_4layers.pt') \n",
        "#load state dict\n",
        "model.load_state_dict(checkpoint['model_state_dict']) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "tBRRVsWqlIoQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc4e512e-a567-420f-8d77-3bbcf41c6161"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 4.7 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.97\n",
            "--2022-11-14 21:39:26--  https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/sentencepiece.french.model\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115362 (1.1M) [application/octet-stream]\n",
            "Saving to: ‘sentencepiece.french.model’\n",
            "\n",
            "sentencepiece.frenc 100%[===================>]   1.06M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2022-11-14 21:39:27 (19.7 MB/s) - ‘sentencepiece.french.model’ saved [1115362/1115362]\n",
            "\n",
            "['▁Bonjour', '▁les', '▁amis', '!']\n",
            "Bonjour les amis!\n"
          ]
        }
      ],
      "source": [
        "!pip install sentencepiece   # uncomment this if you are using google colab\n",
        "!wget https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/sentencepiece.french.model\n",
        "\n",
        "import sentencepiece as spm\n",
        "\n",
        "s = spm.SentencePieceProcessor(model_file='sentencepiece.french.model') #load sentencepiece model\n",
        "\n",
        "#examples\n",
        "encoded = s.encode_as_pieces(\"Bonjour les amis!\")\n",
        "decoded = s.decode_pieces(encoded)\n",
        "print(encoded)\n",
        "print(decoded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "TtLlV05pkQI3"
      },
      "outputs": [],
      "source": [
        "def infer_next_token(sent):\n",
        "    model.eval()\n",
        "    sent_pieces = s.encode_as_pieces(sent)\n",
        "    source = [token2ind['<sos>']] + [token2ind[el] for el in sent_pieces ] # list of tokens\n",
        "    source = torch.tensor(source).to(device)\n",
        "    source = source.reshape(-1, 1)\n",
        "    src_mask = model.base.generate_square_subsequent_mask(source.size(0)).to(device)\n",
        "    out = model(source, src_mask)\n",
        "    next_token_ind =  torch.argmax(out[-1]).item()#fill me\n",
        "    return next_token_ind, out\n",
        "    \n",
        "def infer_next_tokens(sent, max_len=50):\n",
        "    # to be implemented\n",
        "    l = len(s.encode_as_pieces(sent))\n",
        "    while l < max_len:\n",
        "      next_token_ind, _ = infer_next_token(sent)\n",
        "      encoded = s.encode_as_pieces(sent)\n",
        "      encoded.append(ind2token[next_token_ind])\n",
        "      sent = s.decode_pieces(encoded)\n",
        "      l += 1\n",
        "      if ind2token[next_token_ind] == \"<eos>\":\n",
        "        break\n",
        "    return sent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "f83Nn5nSly4v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "95fbe811-b83c-408f-eb3d-60debee41453"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Bonjour les gens qui ont été très accueillants et sympathiques.<eos>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "sent = \"Bonjour les\"\n",
        "infer_next_tokens(sent)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lp7mjVzomoZ3"
      },
      "source": [
        "### Supervised task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "0K1BZsblmEmx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6210a88c-50d4-415e-bd93-096f4ac97034"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-11-14 21:39:27--  https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/cls-books/train.review.spm\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1495960 (1.4M) [text/plain]\n",
            "Saving to: ‘train.review.spm’\n",
            "\n",
            "train.review.spm    100%[===================>]   1.43M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2022-11-14 21:39:27 (22.3 MB/s) - ‘train.review.spm’ saved [1495960/1495960]\n",
            "\n",
            "--2022-11-14 21:39:27--  https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/cls-books/train.label\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3200 (3.1K) [text/plain]\n",
            "Saving to: ‘train.label’\n",
            "\n",
            "train.label         100%[===================>]   3.12K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-11-14 21:39:27 (65.6 MB/s) - ‘train.label’ saved [3200/3200]\n",
            "\n",
            "--2022-11-14 21:39:28--  https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/cls-books/test.review.spm\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1864544 (1.8M) [text/plain]\n",
            "Saving to: ‘test.review.spm’\n",
            "\n",
            "test.review.spm     100%[===================>]   1.78M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2022-11-14 21:39:28 (29.7 MB/s) - ‘test.review.spm’ saved [1864544/1864544]\n",
            "\n",
            "--2022-11-14 21:39:28--  https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/cls-books/test.label\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4000 (3.9K) [text/plain]\n",
            "Saving to: ‘test.label’\n",
            "\n",
            "test.label          100%[===================>]   3.91K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-11-14 21:39:28 (77.3 MB/s) - ‘test.label’ saved [4000/4000]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/cls-books/train.review.spm\n",
        "!wget https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/cls-books/train.label\n",
        "!wget https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/cls-books/test.review.spm\n",
        "!wget https://raw.githubusercontent.com/moussaKam/transfer_learning_transformers/main/cls-books/test.label\n",
        "\n",
        "path_data_train = \"train.review.spm\"\n",
        "path_labels_train = \"train.label\"\n",
        "\n",
        "path_data_valid = \"test.review.spm\"\n",
        "path_labels_valid = \"test.label\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "_MLfvjiom2SL"
      },
      "outputs": [],
      "source": [
        "# a function to evaluate the validation accuracy of the model.\n",
        "def evaluate_accuracy(data_loader):\n",
        "    #to be implemented\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    ncorrect = ntotal = 0\n",
        "    with torch.no_grad():\n",
        "        for idx, data in enumerate(data_loader):\n",
        "            src_mask = model.base.generate_square_subsequent_mask(data[0].size(0)).to(\n",
        "                device\n",
        "            )\n",
        "            output = model(data[0].to(device), src_mask)\n",
        "            label = data[1].to(device)\n",
        "            output = output[-1]\n",
        "            predictions = torch.argmax(output, axis=1)\n",
        "            #print(output.shape, label.shape)\n",
        "            #print(output, label)\n",
        "            # total number of examples\n",
        "            ntotal +=  output.shape[0]\n",
        "            # number of correct predictions \n",
        "            ncorrect += torch.sum(predictions==data[1].to(device))\n",
        "        acc = ncorrect.item() / ntotal\n",
        "        print(\"validation accuracy: {:3.2f}\".format(acc*100))\n",
        "        return acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "qzmx7T7xoa6v"
      },
      "outputs": [],
      "source": [
        "#save the base model to be loaded later in the fine-tuning phase\n",
        "torch.save({\"model_state_dict\": model.base.state_dict(),}, \"pretrained_model_4layers_no_class_head.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "i-xclMCpnVpw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23c10c02-2384-40fb-b5af-91062b6ce519"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=====Trainig FROM SCRATCH======\n",
            "| epoch   1 |    50/  200 steps | loss 0.81249 | ppl    2.254\n",
            "| epoch   1 |   100/  200 steps | loss 0.71985 | ppl    2.054\n",
            "| epoch   1 |   150/  200 steps | loss 0.72980 | ppl    2.075\n",
            "validation accuracy: 60.85\n",
            "| epoch   2 |    50/  200 steps | loss 0.72505 | ppl    2.065\n",
            "| epoch   2 |   100/  200 steps | loss 0.60274 | ppl    1.827\n",
            "| epoch   2 |   150/  200 steps | loss 0.52866 | ppl    1.697\n",
            "validation accuracy: 72.45\n",
            "| epoch   3 |    50/  200 steps | loss 0.38715 | ppl    1.473\n",
            "| epoch   3 |   100/  200 steps | loss 0.40392 | ppl    1.498\n",
            "| epoch   3 |   150/  200 steps | loss 0.42550 | ppl    1.530\n",
            "validation accuracy: 75.00\n",
            "| epoch   4 |    50/  200 steps | loss 0.16989 | ppl    1.185\n",
            "| epoch   4 |   100/  200 steps | loss 0.18130 | ppl    1.199\n",
            "| epoch   4 |   150/  200 steps | loss 0.13025 | ppl    1.139\n",
            "validation accuracy: 73.60\n",
            "| epoch   5 |    50/  200 steps | loss 0.11756 | ppl    1.125\n",
            "| epoch   5 |   100/  200 steps | loss 0.04683 | ppl    1.048\n",
            "| epoch   5 |   150/  200 steps | loss 0.02255 | ppl    1.023\n",
            "validation accuracy: 72.95\n",
            "| epoch   6 |    50/  200 steps | loss 0.00317 | ppl    1.003\n",
            "| epoch   6 |   100/  200 steps | loss 0.00088 | ppl    1.001\n",
            "| epoch   6 |   150/  200 steps | loss 0.00326 | ppl    1.003\n",
            "validation accuracy: 75.50\n",
            "| epoch   7 |    50/  200 steps | loss 0.00003 | ppl    1.000\n",
            "| epoch   7 |   100/  200 steps | loss 0.00797 | ppl    1.008\n",
            "| epoch   7 |   150/  200 steps | loss 0.00010 | ppl    1.000\n",
            "validation accuracy: 75.55\n",
            "| epoch   8 |    50/  200 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch   8 |   100/  200 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch   8 |   150/  200 steps | loss 0.00001 | ppl    1.000\n",
            "validation accuracy: 75.80\n",
            "| epoch   9 |    50/  200 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch   9 |   100/  200 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch   9 |   150/  200 steps | loss 0.00001 | ppl    1.000\n",
            "validation accuracy: 75.75\n",
            "| epoch  10 |    50/  200 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch  10 |   100/  200 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch  10 |   150/  200 steps | loss 0.00001 | ppl    1.000\n",
            "validation accuracy: 75.45\n",
            "| epoch  11 |    50/  200 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch  11 |   100/  200 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch  11 |   150/  200 steps | loss 0.00001 | ppl    1.000\n",
            "validation accuracy: 74.85\n",
            "| epoch  12 |    50/  200 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch  12 |   100/  200 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch  12 |   150/  200 steps | loss 0.00001 | ppl    1.000\n",
            "validation accuracy: 75.70\n",
            "| epoch  13 |    50/  200 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch  13 |   100/  200 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch  13 |   150/  200 steps | loss 0.00000 | ppl    1.000\n",
            "validation accuracy: 75.45\n",
            "| epoch  14 |    50/  200 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch  14 |   100/  200 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  14 |   150/  200 steps | loss 0.00000 | ppl    1.000\n",
            "validation accuracy: 75.60\n",
            "| epoch  15 |    50/  200 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  15 |   100/  200 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  15 |   150/  200 steps | loss 0.00000 | ppl    1.000\n",
            "validation accuracy: 74.80\n",
            "\n",
            "=====PRETRAINED MODEL======\n",
            "| epoch   1 |    50/  200 steps | loss 0.81161 | ppl    2.252\n",
            "| epoch   1 |   100/  200 steps | loss 0.65381 | ppl    1.923\n",
            "| epoch   1 |   150/  200 steps | loss 0.62632 | ppl    1.871\n",
            "validation accuracy: 76.80\n",
            "| epoch   2 |    50/  200 steps | loss 0.45693 | ppl    1.579\n",
            "| epoch   2 |   100/  200 steps | loss 0.47285 | ppl    1.605\n",
            "| epoch   2 |   150/  200 steps | loss 0.40095 | ppl    1.493\n",
            "validation accuracy: 78.15\n",
            "| epoch   3 |    50/  200 steps | loss 0.35234 | ppl    1.422\n",
            "| epoch   3 |   100/  200 steps | loss 0.37950 | ppl    1.462\n",
            "| epoch   3 |   150/  200 steps | loss 0.40539 | ppl    1.500\n",
            "validation accuracy: 79.20\n",
            "| epoch   4 |    50/  200 steps | loss 0.30476 | ppl    1.356\n",
            "| epoch   4 |   100/  200 steps | loss 0.25920 | ppl    1.296\n",
            "| epoch   4 |   150/  200 steps | loss 0.32015 | ppl    1.377\n",
            "validation accuracy: 79.55\n",
            "| epoch   5 |    50/  200 steps | loss 0.16637 | ppl    1.181\n",
            "| epoch   5 |   100/  200 steps | loss 0.13049 | ppl    1.139\n",
            "| epoch   5 |   150/  200 steps | loss 0.42268 | ppl    1.526\n",
            "validation accuracy: 79.35\n",
            "| epoch   6 |    50/  200 steps | loss 0.11122 | ppl    1.118\n",
            "| epoch   6 |   100/  200 steps | loss 0.05791 | ppl    1.060\n",
            "| epoch   6 |   150/  200 steps | loss 0.13258 | ppl    1.142\n",
            "validation accuracy: 79.50\n",
            "| epoch   7 |    50/  200 steps | loss 0.04722 | ppl    1.048\n",
            "| epoch   7 |   100/  200 steps | loss 0.03728 | ppl    1.038\n",
            "| epoch   7 |   150/  200 steps | loss 0.04087 | ppl    1.042\n",
            "validation accuracy: 79.00\n",
            "| epoch   8 |    50/  200 steps | loss 0.02705 | ppl    1.027\n",
            "| epoch   8 |   100/  200 steps | loss 0.00409 | ppl    1.004\n",
            "| epoch   8 |   150/  200 steps | loss 0.01343 | ppl    1.014\n",
            "validation accuracy: 79.30\n",
            "| epoch   9 |    50/  200 steps | loss 0.02747 | ppl    1.028\n",
            "| epoch   9 |   100/  200 steps | loss 0.00539 | ppl    1.005\n",
            "| epoch   9 |   150/  200 steps | loss 0.00013 | ppl    1.000\n",
            "validation accuracy: 78.50\n",
            "| epoch  10 |    50/  200 steps | loss 0.01085 | ppl    1.011\n",
            "| epoch  10 |   100/  200 steps | loss 0.00018 | ppl    1.000\n",
            "| epoch  10 |   150/  200 steps | loss 0.00540 | ppl    1.005\n",
            "validation accuracy: 79.05\n",
            "| epoch  11 |    50/  200 steps | loss 0.00003 | ppl    1.000\n",
            "| epoch  11 |   100/  200 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  11 |   150/  200 steps | loss 0.00000 | ppl    1.000\n",
            "validation accuracy: 79.35\n",
            "| epoch  12 |    50/  200 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch  12 |   100/  200 steps | loss 0.00114 | ppl    1.001\n",
            "| epoch  12 |   150/  200 steps | loss 0.00071 | ppl    1.001\n",
            "validation accuracy: 78.85\n",
            "| epoch  13 |    50/  200 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch  13 |   100/  200 steps | loss 0.00000 | ppl    1.000\n",
            "| epoch  13 |   150/  200 steps | loss 0.01701 | ppl    1.017\n",
            "validation accuracy: 76.85\n",
            "| epoch  14 |    50/  200 steps | loss 0.00061 | ppl    1.001\n",
            "| epoch  14 |   100/  200 steps | loss 0.00089 | ppl    1.001\n",
            "| epoch  14 |   150/  200 steps | loss 0.00380 | ppl    1.004\n",
            "validation accuracy: 79.30\n",
            "| epoch  15 |    50/  200 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch  15 |   100/  200 steps | loss 0.00001 | ppl    1.000\n",
            "| epoch  15 |   150/  200 steps | loss 0.02959 | ppl    1.030\n",
            "validation accuracy: 78.40\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from_scratch_settings = [True, False]\n",
        "\n",
        "from_scratch_valid_acc = []\n",
        "pretrained_valid_acc = []\n",
        "lr = 0.0001\n",
        "\n",
        "for from_scratch in from_scratch_settings:\n",
        "    model = Model(ntokens, nhead, nhid, nlayers, 2, dropout).to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    if not from_scratch:\n",
        "        print(\"=====PRETRAINED MODEL======\")\n",
        "        #load checkpoint\n",
        "        checkpoint = torch.load(\"pretrained_model_4layers_no_class_head.pt\")\n",
        "        #load state dict\n",
        "        model.base.load_state_dict(checkpoint['model_state_dict'])\n",
        "    else:\n",
        "        print(\"=====Trainig FROM SCRATCH======\")\n",
        "    epochs = 15\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        train(\n",
        "            path_data_train,\n",
        "            path_labels_train,\n",
        "            save_interval=-1,\n",
        "            task='classification',\n",
        "            batch_size=8,\n",
        "            log_interval=50,\n",
        "        )\n",
        "        acc = evaluate_accuracy(\n",
        "            get_loader(\n",
        "                path_data_valid,\n",
        "                path_labels_valid,\n",
        "                token2ind=token2ind,\n",
        "                batch_size=20,\n",
        "                task='classification',\n",
        "            )\n",
        "        )\n",
        "        if from_scratch:\n",
        "            from_scratch_valid_acc.append(acc)\n",
        "        else:\n",
        "            pretrained_valid_acc.append(acc)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "RCpBIdTHojm6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "005e9b9a-5af8-42e8-c5d1-fca47a9b1393"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fc060647d90>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "#Visualize the accuracy\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.plot(from_scratch_valid_acc, label='From Scratch Validation Accuracy')\n",
        "plt.plot(pretrained_valid_acc, label='Pretrained Validation Accuracy')\n",
        "plt.legend()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.6.15 ('altegrad')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.15"
    },
    "vscode": {
      "interpreter": {
        "hash": "1f3cfdeab8dd8f9900bd16266619de191cf0f5e09365d74b1fba1714dce58066"
      }
    },
    "gpuClass": "premium"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}