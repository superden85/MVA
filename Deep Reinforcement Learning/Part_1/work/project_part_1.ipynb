{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYW2YAMZOX4-"
      },
      "source": [
        "# **Project**: Maze Rider"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPDPROmXFwU6"
      },
      "source": [
        "**Expectation:**\n",
        "\n",
        "For this project, we require you to do two things:\n",
        "1. Answer the two questions that you will find below. These questions consist in implementing from scratch both a _Model free tabular RL agent_ and a _Deep Reinforcement Learning_ agent on a relatively simple maze environment.\n",
        "2. Write a **short** report (2 pages maximum) on your findings, which algorithm worked best, what hyperparameters where key, what hyperparameters were useless.\n",
        "\n",
        "The following requirements must be met for each projects:\n",
        "- The deadline for the project is on FRIDAY 3 MARCH. You should send us a mail with your project attached as a zip before sunday, midnight. Any additional day over the deadline will reduce the final grade by 2 points. Monday 00:01am counts as one day over the deadline.\n",
        "- Projects should be done in groups of 5, and the following [spreadsheet](https://docs.google.com/spreadsheets/d/1YsgkQwBu1Y-A-y4Bf9JVpy0q4nqI4cOvQkS4LKjnLNg/edit?usp=sharing) should be filled to notify us of the groups.\n",
        "- You can rely as much as you want on the previous practicals, including by copying chunks of code.\n",
        "- You cannot share code with other groups. Any suspicion of code plagiarism will result in us halving the grade for both groups."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AQJJ2OP30pTw"
      },
      "outputs": [],
      "source": [
        "#@title Installation\n",
        "!pip install chex\n",
        "!pip install jax\n",
        "!pip install optax\n",
        "!pip install dm-acme\n",
        "!pip install dm-acme[reverb]\n",
        "#!pip install dm-acme[jax]\n",
        "!pip install dm-acme[tf]\n",
        "!pip install dm-acme[envs]\n",
        "!pip install dm-env\n",
        "!pip install dm-haiku\n",
        "!pip install imageio\n",
        "!pip install gym\n",
        "\n",
        "from IPython.display import clear_output\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bNlEnGsYOX5A"
      },
      "outputs": [],
      "source": [
        "#@title Imports\n",
        "import sys\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import math\n",
        "import time\n",
        "import numpy as np\n",
        "from gym import utils\n",
        "import enum\n",
        "from typing import *\n",
        "import chex\n",
        "from dataclasses import dataclass, asdict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOQiLjWr6ihq"
      },
      "source": [
        "# Environment\n",
        "\n",
        "For this project, our environment will be a maze. The idea is pretty simple: our agent starts at one end of the maze and must reach a certain goal at the other end. To make things spicier, the agent must also avoid pits scattered in the environment.\n",
        "\n",
        "At each step the agent can perform one of this four actions: move to the left, to the right, up or down:\n",
        "- If the agent reaches an empty cell, it gets a reward of $0$.\n",
        "- If the agent tries to cross a wall, nothing happens and it also gets a reward of $0$.\n",
        "- If the agent reaches the goal, the episode ends and it gets a reward of $1$.\n",
        "- If the agent falls into a pit, the episode ends and it gets a reward of $-1$.\n",
        "\n",
        "The walls will stay the same between each episode, however, the agent's starting position as well as the positions of the goal and the pits will be randomly picked between a small number of candidates at each episode."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sHqwloB5jDiI"
      },
      "outputs": [],
      "source": [
        "# @title **[Skip]** Environment implementation details\n",
        "\n",
        "TXT_TEMPLATE = \"\"\"\n",
        "#######  #######\n",
        "# SSS #  #PGGGP#\n",
        "### ###  ##P P##\n",
        "  # ######## #\n",
        "  #          #\n",
        "  ############\n",
        "---\n",
        "S=0\n",
        "G=0\n",
        "P=.5\n",
        "\"\"\"\n",
        "\n",
        "BIG_MAZE = \"\"\"\n",
        "#########################################\n",
        "#SSS#     #PPP#S#      #         #G   PP#\n",
        "#   #GP       # #####  #P  #     #      #\n",
        "#   ####  #   # #G     #   #GGGGGGGGG   #\n",
        "#      GGG#   # #      #   ###########  #\n",
        "###########         #      #            #\n",
        "#G        #PP     GGGGGG#GG###S      GGG#\n",
        "#PP  P#   ##################    #########\n",
        "#######   #      #P                     #\n",
        "#G               G      #S      P#      #\n",
        "#########################################\n",
        "---\n",
        "S=0\n",
        "G=0\n",
        "P=0.5\n",
        "\"\"\"\n",
        "\n",
        "class CellType(enum.IntEnum):\n",
        "  WALL = enum.auto()\n",
        "  PLAYER = enum.auto()\n",
        "  GOAL = enum.auto()\n",
        "  PIT = enum.auto()\n",
        "  EMPTY = enum.auto()\n",
        "\n",
        "TYPE_TO_CHAR = {\n",
        "      CellType.WALL: '#',\n",
        "      CellType.PLAYER: 'S',\n",
        "      CellType.GOAL: 'G',\n",
        "      CellType.PIT: 'P',\n",
        "      CellType.EMPTY: ' ',\n",
        "  }\n",
        "\n",
        "def process_template(txt_template: str) -> Tuple[chex.Array, Mapping[str, float]]:\n",
        "  char_to_type = {v: k for k, v in TYPE_TO_CHAR.items()}\n",
        "  maze_template, maze_info = txt_template.split(\"---\")\n",
        "  grid = maze_template.split('\\n')\n",
        "  grid = [l for l in grid if l]\n",
        "  n_lines, n_cols = len(grid), max(map(len, grid))\n",
        "  grid = [l + ' ' * (n_cols - len(l)) for l in grid]\n",
        "  grid = np.array(list(map(lambda x: list(x), grid)))\n",
        "  mask_pattern = np.array([' '] * len(CellType))\n",
        "  for k, v in TYPE_TO_CHAR.items():\n",
        "    mask_pattern[k.value - 1] = v\n",
        "\n",
        "  grid = grid[:, :, None] == mask_pattern[None, None]\n",
        "  maze_infos = [list(map(lambda x: x.strip(' '), x.split('='))) for x in maze_info.split('\\n') if x]\n",
        "  maze_infos = {char_to_type[k]: float(v) for k, v in maze_infos}\n",
        "  return grid, maze_infos\n",
        "\n",
        "def sample_grid(grid: chex.Array, maze_infos: Mapping[str, float]) -> chex.Array:\n",
        "  sample = grid.copy()\n",
        "  for k, v in maze_infos.items():\n",
        "    layer = grid[..., k.value - 1]\n",
        "    if v > 0.:\n",
        "      mask = np.random.binomial(1, v, size=layer.shape)\n",
        "      sample[..., k.value - 1] = mask * layer\n",
        "    else:\n",
        "      idxs = np.nonzero(layer)\n",
        "      i = np.random.randint(idxs[0].shape[0])\n",
        "      sample[..., k.value - 1] = np.zeros_like(sample[..., k.value - 1])\n",
        "      sample[..., k.value - 1][idxs[0][i], idxs[1][i]] = 1\n",
        "\n",
        "  # For cells that have no values, fill the space layer with 1\n",
        "  sample[..., CellType.EMPTY.value - 1] = np.maximum(np.all(sample == 0, axis=-1), sample[..., CellType.EMPTY.value - 1])\n",
        "  return sample\n",
        "\n",
        "def to_string(grid: chex.Array) -> str:\n",
        "  mask_pattern = np.array([' '] * len(CellType))\n",
        "  for k, v in TYPE_TO_CHAR.items():\n",
        "    mask_pattern[k.value - 1] = v\n",
        "  str_grid = mask_pattern[grid.argmax(axis=-1)]\n",
        "  str_grid = '\\n'.join(map(lambda x: ''.join(x), str_grid))\n",
        "  return str_grid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMQYoeWEJJpJ"
      },
      "source": [
        "Read the following definition of the `Maze` environment, so that you get an idea of what you have access to in the environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vAYm2B02-V9G"
      },
      "outputs": [],
      "source": [
        "# @title **[Read]** Environment definition\n",
        "\n",
        "class Maze:\n",
        "  \"\"\"Definition of the Maze environment.\n",
        "\n",
        "  You do not need to read this class implementation in detail, you should\n",
        "  just have a look at its different methods, to see what properties of the\n",
        "  environment you can access.\n",
        "  \"\"\"\n",
        "  def __init__(self, txt_template: str) -> None:\n",
        "    self._grid_template, self._maze_infos = process_template(txt_template)\n",
        "\n",
        "  def reset(self) -> chex.Array:\n",
        "    \"\"\"Resets the environment.\"\"\"\n",
        "    self._grid = sample_grid(self._grid_template, self._maze_infos)\n",
        "    return self._grid\n",
        "\n",
        "  def step(self, action: int) -> Tuple[chex.Array, float, bool]:\n",
        "    \"\"\"Perfoms one step in the environment.\n",
        "\n",
        "    Args:\n",
        "      action: index of the action to perform.\n",
        "    Returns:\n",
        "      next_state, reward, done\n",
        "\n",
        "    next_state: next state of the environment.\n",
        "    reward: reward of the current step.\n",
        "    done: if True the game has ended.\n",
        "    \"\"\"\n",
        "    n_rows, n_cols, _ = self._grid.shape\n",
        "    x, y = np.nonzero(self._grid[..., CellType.PLAYER.value - 1])\n",
        "    x, y = x[0], y[0]\n",
        "    next_x, next_y = x, y\n",
        "    if action == 0:\n",
        "      next_x = max(min(x + 1, n_rows - 1), 0)\n",
        "    elif action == 1:\n",
        "      next_x = max(min(x - 1, n_rows - 1), 0)\n",
        "    elif action == 2:\n",
        "      next_y = max(min(y + 1, n_cols - 1), 0)\n",
        "    elif action == 3:\n",
        "      next_y = max(min(y - 1, n_cols - 1), 0)\n",
        "    next_cell_type = CellType(self._grid[next_x, next_y].argmax() + 1)\n",
        "    if next_cell_type is CellType.WALL:\n",
        "      next_x, next_y = x, y\n",
        "      reward = 0.\n",
        "      done = False\n",
        "    elif next_cell_type is CellType.EMPTY:\n",
        "      reward = 0.\n",
        "      done = False\n",
        "    elif next_cell_type == CellType.PIT:\n",
        "      reward = -1.\n",
        "      done = True\n",
        "    elif next_cell_type == CellType.GOAL:\n",
        "      reward = 1.\n",
        "      done = True\n",
        "    else:\n",
        "      raise ValueError(f'Unknown next cell type {next_cell_type}, this should not happen.')\n",
        "    self._grid[..., CellType.PLAYER.value - 1][x, y] = 0\n",
        "    self._grid[..., CellType.EMPTY.value - 1][x, y] = 1\n",
        "    self._grid[..., :][next_x, next_y] = 0\n",
        "    self._grid[..., CellType.PLAYER.value - 1][next_x, next_y] = 1\n",
        "    return self._grid, reward, done\n",
        "\n",
        "  def render(self) -> str:\n",
        "    \"\"\"Renders the environment as a string.\"\"\"\n",
        "    return to_string(self._grid)\n",
        "\n",
        "  def num_actions(self) -> int:\n",
        "    \"\"\"Returns the number of usable actions.\"\"\"\n",
        "    return 4\n",
        "\n",
        "  def obs_shape(self) -> Tuple[int, int]:\n",
        "    \"\"\"Returns the shape of the Maze.\"\"\"\n",
        "    return self._grid.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qj97i0eH4SFW"
      },
      "source": [
        "Use the following cell to interact manually with the environment: launch it and a small command line will appear. The commands **U**, **D**, **L** and **R** will move the agent `S`, try to reach the goal `G` by hand while avoiding the eventual pits `P`.\n",
        "\n",
        "If, for some reason, you want to stop the episode before its end just enter **STOP** into the command line."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HHtrMGmc-pYb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcf661f1-09c1-458c-aff0-e46a4a438835",
        "cellView": "form"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#######  #######\n",
            "#   S #  # G  P#\n",
            "### ###  ##P  ##\n",
            "  # ######## #  \n",
            "  #          #  \n",
            "  ############  \n",
            "STOP\n"
          ]
        }
      ],
      "source": [
        "#@title **[Play with the environment]**\n",
        "env = Maze(TXT_TEMPLATE)\n",
        "\n",
        "env.reset()\n",
        "\n",
        "action_to_int = {\n",
        "    'D': 0,\n",
        "    'U': 1,\n",
        "    'R': 2,\n",
        "    'L': 3,\n",
        "}\n",
        "\n",
        "# To stop the game, enter this command\n",
        "STOP_GAME = 'STOP'\n",
        "\n",
        "while True:\n",
        "  print(env.render())\n",
        "  while True:\n",
        "    action = input().upper()\n",
        "    if action in ['U', 'D', 'L', 'R', STOP_GAME]:\n",
        "      break\n",
        "  if action == STOP_GAME:\n",
        "    break\n",
        "  _, reward, done = env.step(action_to_int[action])\n",
        "  print(f'-----\\nReward: {reward}\\n-----\\n')\n",
        "  if done:\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-wgbwz284MI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1216010a-ce15-4b68-a0d3-4d958fa1be01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(6, 16, 5)\n",
            "[[False False False False False False False False False False False False\n",
            "  False False False False]\n",
            " [False False False False False False False False False False False False\n",
            "  False False False False]\n",
            " [False False False False False False False False False False False  True\n",
            "  False  True False False]\n",
            " [False False False False False False False False False False False False\n",
            "  False False False False]\n",
            " [False False False False False False False False False False False False\n",
            "  False False False False]\n",
            " [False False False False False False False False False False False False\n",
            "  False False False False]]\n"
          ]
        }
      ],
      "source": [
        "environment = Maze(TXT_TEMPLATE)\n",
        "state = environment.reset()\n",
        "print(state.shape)\n",
        "print(state[:, :, 3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgWMSWNOvr73"
      },
      "source": [
        "# **[Exercice 1]** Implementing a tabular RL Algorithm.\n",
        "\n",
        "Your first task is to implement a model free tabular RL algorithm for the maze environment. You can go for either Q-learning or SARSA. Feel free to take inspiration from the other practicals.\n",
        "\n",
        "## Hashing\n",
        "As in the DQN practical, the environment state is a `chex.Array`, and it can thus not directly be put in a dictionary. As in this practical, you should implement a `numpy_to_hash` function.\n",
        "\n",
        "This function should transform each state into something that can be used as a key in a dictionary. Additionally, this function should be invertible (each input is uniquely mapped to one unique output). In other word, this function should not lose any information. Think about all the information that are contained in a state (position of the agent, position of the goal, ...) and try to encode them in a hash in a compact way.\n",
        "\n",
        "Contrary to what was done in the DQN practical, you do not need to implement a `hash_to_numpy` function that is the inverse of `np_to_hash`.\n",
        "\n",
        "## Agent API\n",
        "Your agent must implement the following functions:\n",
        "- `def act(self, state: chex.Array, eval: bool = False) -> int` a function that takes in an observation and returns the action selected by the agent. `eval=True` means that you should use the learnt policy, `eval=False` means that you should use the acting policy. Depending on your choice of agent, these two policies may or may not differ.\n",
        "- `def update(self, state: chex.Array, action: int, next_state: chex.Array, reward: float, done: bool) -> None:` a function that takes in a transition and updates the agent policy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_g1mEl3Jxd1p"
      },
      "outputs": [],
      "source": [
        "def numpy_to_hash(x: chex.Array) -> str:\n",
        "    return x.tobytes()\n",
        "\n",
        "def hash_to_numpy(h: str, env: Maze) -> chex.Array:\n",
        "  return np.frombuffer(h).reshape(env.obs_shape())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yv0jY1uAOULn"
      },
      "outputs": [],
      "source": [
        "# @title **[Implement]** Define your agent\n",
        "class Agent:\n",
        "  def __init__(self, env, gamma, epsilon, alpha, default_qvalue=0):\n",
        "    self._Na = env.num_actions()\n",
        "    self._gamma = gamma\n",
        "    self._epsilon = epsilon\n",
        "    self._alpha = alpha\n",
        "    self._default_qvalue = default_qvalue\n",
        "    self.q_values = dict()\n",
        "\n",
        "  def act(self, state: chex.Array, eval: bool = False) -> int:\n",
        "    s = numpy_to_hash(state)\n",
        "    if (not eval and np.random.rand() < self._epsilon) or s not in self.q_values:\n",
        "      return np.random.randint(self._Na)\n",
        "    \n",
        "    return np.argmax(self.q_values[s])\n",
        "    \n",
        "\n",
        "  def update(self, state: chex.Array, action: int, next_state: chex.Array, reward: float, done: bool) -> None:\n",
        "    \n",
        "    s1 = numpy_to_hash(state)\n",
        "    if s1 not in self.q_values:\n",
        "      self.q_values[s1] = np.ones((self._Na,)) * self._default_qvalue\n",
        "\n",
        "    s2 = numpy_to_hash(next_state)\n",
        "    if s2 not in self.q_values:\n",
        "      self.q_values[s2] = np.ones((self._Na,)) * self._default_qvalue\n",
        "\n",
        "    next_value = np.max(self.q_values[s2]) if not done else 0\n",
        "    self.q_values[s1][action] += self._alpha * (reward + self._gamma * next_value - self.q_values[s1][action]) "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We went for a Q-Learning procedure. The chosen Maze is the little one as then the space and actions space is small enough to justify tabular RL."
      ],
      "metadata": {
        "id": "aEm0TOlXl1si"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ESGaxWdSET05",
        "outputId": "79d3fbab-1025-46f4-a6d2-dc9dad1f25e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode number:\t| Average reward on 100 eval episodes\n",
            "------------------------------------------------------\n",
            "\t0\t|\t-0.3\n",
            "\t10\t|\t0.0\n",
            "\t20\t|\t-0.1\n",
            "\t30\t|\t0.2\n",
            "\t40\t|\t0.2\n",
            "\t50\t|\t-0.1\n",
            "\t60\t|\t0.3\n",
            "\t70\t|\t0.0\n",
            "\t80\t|\t0.0\n",
            "\t90\t|\t0.1\n",
            "\t100\t|\t0.1\n",
            "\t110\t|\t0.0\n",
            "\t120\t|\t-0.1\n",
            "\t130\t|\t0.0\n",
            "\t140\t|\t0.0\n",
            "\t150\t|\t0.0\n",
            "\t160\t|\t0.0\n",
            "\t170\t|\t0.0\n",
            "\t180\t|\t0.0\n",
            "\t190\t|\t0.0\n",
            "\t200\t|\t0.0\n",
            "\t210\t|\t0.0\n",
            "\t220\t|\t0.0\n",
            "\t230\t|\t0.0\n",
            "\t240\t|\t0.0\n",
            "\t250\t|\t0.0\n",
            "\t260\t|\t0.0\n",
            "\t270\t|\t0.0\n",
            "\t280\t|\t0.0\n",
            "\t290\t|\t0.0\n",
            "\t300\t|\t0.0\n",
            "\t310\t|\t0.0\n",
            "\t320\t|\t0.0\n",
            "\t330\t|\t0.0\n",
            "\t340\t|\t0.0\n",
            "\t350\t|\t0.0\n",
            "\t360\t|\t0.0\n",
            "\t370\t|\t0.0\n",
            "\t380\t|\t0.0\n",
            "\t390\t|\t0.0\n",
            "\t400\t|\t0.0\n",
            "\t410\t|\t0.0\n",
            "\t420\t|\t0.0\n",
            "\t430\t|\t0.0\n",
            "\t440\t|\t0.0\n",
            "\t450\t|\t0.0\n",
            "\t460\t|\t0.0\n",
            "\t470\t|\t0.1\n",
            "\t480\t|\t0.0\n",
            "\t490\t|\t0.0\n",
            "\t500\t|\t0.0\n",
            "\t510\t|\t0.1\n",
            "\t520\t|\t0.0\n",
            "\t530\t|\t0.0\n",
            "\t540\t|\t0.3\n",
            "\t550\t|\t0.0\n",
            "\t560\t|\t0.1\n",
            "\t570\t|\t0.0\n",
            "\t580\t|\t0.0\n",
            "\t590\t|\t0.1\n",
            "\t600\t|\t0.1\n",
            "\t610\t|\t0.2\n",
            "\t620\t|\t0.2\n",
            "\t630\t|\t0.1\n",
            "\t640\t|\t0.2\n",
            "\t650\t|\t0.1\n",
            "\t660\t|\t0.3\n",
            "\t670\t|\t0.1\n",
            "\t680\t|\t0.1\n",
            "\t690\t|\t0.2\n",
            "\t700\t|\t0.3\n",
            "\t710\t|\t0.2\n",
            "\t720\t|\t0.2\n",
            "\t730\t|\t0.4\n",
            "\t740\t|\t0.2\n",
            "\t750\t|\t0.3\n",
            "\t760\t|\t0.4\n",
            "\t770\t|\t0.1\n",
            "\t780\t|\t0.2\n",
            "\t790\t|\t0.1\n",
            "\t800\t|\t0.1\n",
            "\t810\t|\t0.1\n",
            "\t820\t|\t0.3\n",
            "\t830\t|\t0.1\n",
            "\t840\t|\t0.3\n",
            "\t850\t|\t0.1\n",
            "\t860\t|\t0.1\n",
            "\t870\t|\t0.2\n",
            "\t880\t|\t0.5\n",
            "\t890\t|\t0.2\n",
            "\t900\t|\t0.0\n",
            "\t910\t|\t0.4\n",
            "\t920\t|\t0.3\n",
            "\t930\t|\t0.2\n",
            "\t940\t|\t0.3\n",
            "\t950\t|\t0.3\n",
            "\t960\t|\t0.4\n",
            "\t970\t|\t0.6\n",
            "\t980\t|\t0.2\n",
            "\t990\t|\t0.2\n",
            "\t1000\t|\t0.0\n",
            "\t1010\t|\t0.1\n",
            "\t1020\t|\t0.1\n",
            "\t1030\t|\t0.3\n",
            "\t1040\t|\t0.6\n",
            "\t1050\t|\t0.3\n",
            "\t1060\t|\t0.6\n",
            "\t1070\t|\t0.2\n",
            "\t1080\t|\t0.3\n",
            "\t1090\t|\t0.5\n",
            "\t1100\t|\t0.5\n",
            "\t1110\t|\t0.5\n",
            "\t1120\t|\t0.5\n",
            "\t1130\t|\t0.2\n",
            "\t1140\t|\t0.4\n",
            "\t1150\t|\t0.4\n",
            "\t1160\t|\t0.7\n",
            "\t1170\t|\t0.4\n",
            "\t1180\t|\t0.5\n",
            "\t1190\t|\t0.6\n",
            "\t1200\t|\t0.4\n",
            "\t1210\t|\t0.7\n",
            "\t1220\t|\t0.6\n",
            "\t1230\t|\t0.5\n",
            "\t1240\t|\t0.4\n",
            "\t1250\t|\t0.3\n",
            "\t1260\t|\t0.8\n",
            "\t1270\t|\t0.5\n",
            "\t1280\t|\t0.5\n",
            "\t1290\t|\t0.5\n",
            "\t1300\t|\t0.7\n",
            "\t1310\t|\t0.7\n",
            "\t1320\t|\t0.6\n",
            "\t1330\t|\t0.7\n",
            "\t1340\t|\t0.5\n",
            "\t1350\t|\t0.4\n",
            "\t1360\t|\t0.6\n",
            "\t1370\t|\t0.5\n",
            "\t1380\t|\t0.4\n",
            "\t1390\t|\t0.8\n",
            "\t1400\t|\t0.7\n",
            "\t1410\t|\t0.3\n",
            "\t1420\t|\t0.6\n",
            "\t1430\t|\t0.6\n",
            "\t1440\t|\t0.5\n",
            "\t1450\t|\t0.6\n",
            "\t1460\t|\t0.7\n",
            "\t1470\t|\t0.5\n",
            "\t1480\t|\t0.5\n",
            "\t1490\t|\t0.7\n",
            "\t1500\t|\t0.7\n",
            "\t1510\t|\t0.7\n",
            "\t1520\t|\t0.7\n",
            "\t1530\t|\t0.4\n",
            "\t1540\t|\t0.8\n",
            "\t1550\t|\t0.8\n",
            "\t1560\t|\t0.7\n",
            "\t1570\t|\t0.8\n",
            "\t1580\t|\t0.7\n",
            "\t1590\t|\t0.6\n",
            "\t1600\t|\t0.6\n",
            "\t1610\t|\t0.6\n",
            "\t1620\t|\t0.8\n",
            "\t1630\t|\t0.7\n",
            "\t1640\t|\t0.4\n",
            "\t1650\t|\t0.7\n",
            "\t1660\t|\t0.9\n",
            "\t1670\t|\t0.8\n",
            "\t1680\t|\t0.7\n",
            "\t1690\t|\t0.8\n",
            "\t1700\t|\t0.8\n",
            "\t1710\t|\t0.4\n",
            "\t1720\t|\t0.9\n",
            "\t1730\t|\t0.9\n",
            "\t1740\t|\t0.7\n",
            "\t1750\t|\t0.5\n",
            "\t1760\t|\t0.8\n",
            "\t1770\t|\t0.7\n",
            "\t1780\t|\t1.0\n",
            "\t1790\t|\t0.9\n",
            "\t1800\t|\t0.9\n",
            "\t1810\t|\t0.7\n",
            "\t1820\t|\t1.0\n",
            "\t1830\t|\t0.9\n",
            "\t1840\t|\t0.8\n",
            "\t1850\t|\t0.7\n",
            "\t1860\t|\t0.6\n",
            "\t1870\t|\t0.7\n",
            "\t1880\t|\t0.9\n",
            "\t1890\t|\t0.8\n",
            "\t1900\t|\t0.8\n",
            "\t1910\t|\t0.8\n",
            "\t1920\t|\t0.9\n",
            "\t1930\t|\t1.0\n",
            "\t1940\t|\t0.9\n",
            "\t1950\t|\t0.8\n",
            "\t1960\t|\t1.0\n",
            "\t1970\t|\t0.9\n",
            "\t1980\t|\t0.9\n",
            "\t1990\t|\t0.7\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fb6eeee1f40>]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# @title **[Fill]** Run your agent\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "num_episodes = 2000\n",
        "num_eval_episodes = 10\n",
        "eval_every_N = 10\n",
        "\n",
        "environment = Maze(TXT_TEMPLATE)\n",
        "# environment = Maze(BIG_MAZE)\n",
        "Na = environment.num_actions()\n",
        "\n",
        "gamma = 0.99\n",
        "epsilon = 0.5\n",
        "alpha = 0.01\n",
        "default_qvalue = 0\n",
        "\n",
        "# [DEFINE YOUR AGENT HERE]\n",
        "agent = Agent(environment, gamma, epsilon, alpha, default_qvalue)\n",
        "all_rewards = []\n",
        "episodes = []\n",
        "\n",
        "def run_episode(agent: Agent, env: Maze, eval: bool, max_steps: int = 1000, render: bool = False) -> float:\n",
        "  # Reset any counts and start the environment.\n",
        "  state = environment.reset()\n",
        "  n_steps = 0\n",
        "\n",
        "  # Run an episode.\n",
        "  while True:\n",
        "    if render:\n",
        "      print(f'Step: {n_steps}')\n",
        "      print(environment.render())\n",
        "\n",
        "    # Generate an action from the agent's policy and step the environment.\n",
        "    action = agent.act(state, eval=eval)\n",
        "    next_state, reward, done = environment.step(action)\n",
        "    if not eval:\n",
        "      agent.update(state, action, next_state, reward, done)\n",
        "    n_steps += 1\n",
        "\n",
        "    state = next_state.copy()\n",
        "    if done or n_steps > max_steps:\n",
        "      break\n",
        "\n",
        "  return reward\n",
        "\n",
        "print(\"Episode number:\\t| Average reward on 100 eval episodes\")\n",
        "print(\"------------------------------------------------------\")\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "  run_episode(agent, environment, eval=False)\n",
        "\n",
        "  if episode % eval_every_N == 0:\n",
        "    reward = np.mean([run_episode(agent, environment, eval=True) for _ in range(num_eval_episodes)])\n",
        "    print(f\"\\t{episode}\\t|\\t{reward}\")\n",
        "    all_rewards.append(reward)\n",
        "    episodes.append(episode)\n",
        "\n",
        "plt.xlabel('Number of training episodes')\n",
        "plt.ylabel('Average return')\n",
        "plt.plot(episodes, all_rewards)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A165AUb6FUvL"
      },
      "outputs": [],
      "source": [
        "# @title **[Run]** Visualize one trajectory for your agent.\n",
        "run_episode(agent, environment, eval=True, render=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PH0ZxZYz8rTz"
      },
      "source": [
        "# **[Exercice 2]** Implementing a deep RL Algorithm.\n",
        "\n",
        "Your second (harder) task is to implement a _Deep RL algorithm_ for the maze environment. You can go for either DQN or a variant of policy gradient of your choice. Again, feel free to take inspiration from the other practicals.\n",
        "\n",
        "## Agent API\n",
        "Your agent must implement the following functions:\n",
        "- `def act(self, state: chex.Array, eval: bool = False) -> int` a function that takes in an observation and returns the action selected by the agent. `eval=True` means that you should use the learnt policy, `eval=False` means that you should use the acting policy. Depending on your choice of agent, these two policies may or may not differ.\n",
        "- ` def first_observe(self, state: chex.Array) -> None:` a function that takes care of handling the first observation obtained after a reset. This function can notably memorize the first state, so that when we call observe, we have a full transition `(state, action, reward, done, next_state)` at our disposal.\n",
        "- `def observe(self, action_t: chex.Array, reward_t: chex.Array, done_t: chex.Array, state_tp1: chex.Array) -> chex.Array:` a function that takes an action, a reward, a termination signal and a next state, and performs any update required by our agent to update its policy. To make things much easier, you should look at what this function is doing for the implementation in the DQN practical."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zGGwsA0CyWm"
      },
      "source": [
        "#Imports for Deep RL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "epna689Nyo0Y"
      },
      "outputs": [],
      "source": [
        "import haiku as hk\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import optax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbBN9IScC27U"
      },
      "source": [
        "#Logging with wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f3q5s7mHC9cm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f21213c5-5266-4180-bf4f-5c4ec282a374"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.13.10-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (8.1.3)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 KB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.16.0-py2.py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 KB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.8/dist-packages (from wandb) (1.4.4)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from wandb) (4.5.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.12.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (3.19.6)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.8/dist-packages (from wandb) (6.0)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.3.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from wandb) (57.4.0)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from wandb) (2.25.1)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.15.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 KB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.14)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.0.0->wandb) (4.0.0)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=a71c70fc2562bafadf445d022fb8d349f5e4a90d307f65a04c813c5b3dae363a\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/8e/7e/72fbc243e1aeecae64a96875432e70d4e92f3d2d18123be004\n",
            "Successfully built pathtools\n",
            "Installing collected packages: pathtools, smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.31 docker-pycreds-0.4.0 gitdb-4.0.10 pathtools-0.1.2 sentry-sdk-1.16.0 setproctitle-1.3.2 smmap-5.0.0 wandb-0.13.10\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMJNOkdMDL-V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d7c0745-fa76-4a46-a3fb-2812c6363e61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "import wandb\n",
        "WANDB_API_KEY = 'aaac06fd950730949910d815f40a98c430390a0b'\n",
        "wandb.login(key=WANDB_API_KEY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KzCvN6PbFI12"
      },
      "outputs": [],
      "source": [
        "project_name = \"deeprl_project_1\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting the optimal trajectories"
      ],
      "metadata": {
        "id": "64wCGF_Pavbu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the maze is quite big, it is nearly impossible for an untrained model to stumble by chance upon the goal with a reasonable number of steps, so convergence was nearly impossible.\n",
        "\n",
        "Fortunately solving the shortest path between the start and the goal is a problem for which we have exact algorithm. Thus we tried to guide the model by feeding him the optimal action at each step.\n",
        "\n",
        "The following function (Breadth First Search) computes a correct shortest path."
      ],
      "metadata": {
        "id": "S6AbxLK4n8ca"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bfs(grid : chex.Array):\n",
        "  m, n, _ = grid.shape\n",
        "  next = [[[None for action in range(env.num_actions())] for y in range(n)] for x in range(m)]\n",
        "  #construction of the adjency matrix:\n",
        "  for x in range(m):\n",
        "    for y in range(n):\n",
        "      #pass if wall, pit, or goal\n",
        "      if grid[..., CellType.WALL.value - 1][x, y] or grid[..., CellType.PIT.value - 1][x, y] or grid[..., CellType.GOAL.value - 1][x, y]:\n",
        "        continue\n",
        "      #it means it is an empty cell, maybe the player is on it\n",
        "      for action in range(env.num_actions()):\n",
        "        next_x, next_y = x, y\n",
        "        if action == 0:\n",
        "          next_x = max(min(x + 1, m - 1), 0)\n",
        "        elif action == 1:\n",
        "          next_x = max(min(x - 1, m - 1), 0)\n",
        "        elif action == 2:\n",
        "          next_y = max(min(y + 1, n - 1), 0)\n",
        "        elif action == 3:\n",
        "          next_y = max(min(y - 1, n - 1), 0)\n",
        "\n",
        "        if grid[..., CellType.WALL.value - 1][next_x, next_y] or grid[..., CellType.PIT.value - 1][next_x, next_y]:\n",
        "          next_x, next_y = x, y\n",
        "        next[x][y][action] = (next_x, next_y)\n",
        "  \n",
        "  xp, yp = np.nonzero(grid[..., CellType.PLAYER.value - 1])\n",
        "  xp, yp = xp[0], yp[0]\n",
        "  xg, yg = np.nonzero(grid[..., CellType.GOAL.value - 1])\n",
        "  xg, yg = xg[0], yg[0]\n",
        "\n",
        "  queue = [(xp, yp)]\n",
        "  treated = [[False for y in range(n)] for x in range(m)]\n",
        "  father = [[None for y in range(n)] for x in range(m)]\n",
        "  while queue and not(x == xg and y == yg):\n",
        "    x, y = queue.pop(0)\n",
        "    for action in range(env.num_actions()):\n",
        "      if next[x][y][action] is None:\n",
        "        continue\n",
        "      next_x, next_y = next[x][y][action]\n",
        "      if not treated[next_x][next_y]:\n",
        "        treated[next_x][next_y] = True\n",
        "        father[next_x][next_y] = (x, y, action)\n",
        "        queue.append((next_x, next_y))\n",
        "\n",
        "  #getting the optimal actions\n",
        "  optimal_actions = [[None for y in range(n)] for x in range(m)]\n",
        "  x, y = xg, yg\n",
        "  while not(x == xp and y == yp):\n",
        "    prev_x, prev_y, action = father[x][y]\n",
        "    optimal_actions[prev_x][prev_y] = action\n",
        "    x, y = prev_x, prev_y\n",
        "  \n",
        "  return optimal_actions, next"
      ],
      "metadata": {
        "id": "3Rskdcgla3C3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClcHHtwSzgpl"
      },
      "source": [
        "#Replay Buffer for DQN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ytlZK_RAzgFY"
      },
      "outputs": [],
      "source": [
        "@chex.dataclass\n",
        "class Transition:\n",
        "  state_t: chex.Array\n",
        "  action_t: chex.Array\n",
        "  reward_t: chex.Array\n",
        "  done_t: chex.Array\n",
        "  state_tp1: chex.Array\n",
        "\n",
        "\n",
        "class ReplayBuffer:\n",
        "  \"\"\"Fixed-size buffer to store transition tuples.\"\"\"\n",
        "\n",
        "  def __init__(self, buffer_capacity: int):\n",
        "      \"\"\"Initialize a ReplayBuffer object.\n",
        "      Args:\n",
        "          batch_size (int): size of each training batch\n",
        "      \"\"\"\n",
        "      self._memory = list()\n",
        "      self._maxlen = buffer_capacity\n",
        "\n",
        "  @property\n",
        "  def size(self) -> int:\n",
        "    # Return the current number of elements in the buffer.\n",
        "    return len(self._memory)\n",
        "\n",
        "  def add(self, state_t: chex.Array,\n",
        "          action_t: chex.Array,\n",
        "          reward_t: chex.Array,\n",
        "          done_t: chex.Array,\n",
        "          state_tp1: chex.Array) -> None:\n",
        "      \"\"\"Add a new transition to memory.\"\"\"\n",
        "\n",
        "      if self.size > self._maxlen:\n",
        "          self._memory = self._memory[1:]\n",
        "      \n",
        "      self._memory.append(Transition(state_t=state_t,\n",
        "                              action_t = action_t,\n",
        "                              reward_t = reward_t,\n",
        "                              done_t = done_t,\n",
        "                              state_tp1 = state_tp1))\n",
        "\n",
        "  def sample(self) -> Transition:\n",
        "      \"\"\"Randomly sample a transition from memory.\"\"\"\n",
        "      assert self._memory, 'replay buffer is unfilled'\n",
        "\n",
        "      index = np.random.randint(self.size)\n",
        "      return self._memory[index]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kcWXpGd-ztpI"
      },
      "outputs": [],
      "source": [
        "class BatchedReplayBuffer(ReplayBuffer):\n",
        "\n",
        "  def sample_batch(self, batch_size) -> Transition:\n",
        "    \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
        "    assert len(self._memory) >= batch_size, 'Insuficient number of transitions in replay buffer'\n",
        "    # Your code here !\n",
        "    samples = [self.sample() for i in range(batch_size)]\n",
        "    kwargs = dict()\n",
        "    for attr in [\"state_t\", \"action_t\", \"reward_t\", \"done_t\", \"state_tp1\"]:\n",
        "        kwargs[attr] = np.array([getattr(s, attr) for s in samples])\n",
        "    return Transition(**kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amiWlYnZUjwC"
      },
      "source": [
        "# Prioritized Experience Replay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "isY5-Wq7UmQb"
      },
      "outputs": [],
      "source": [
        "class PrioritizedReplayBuffer:\n",
        "  \"\"\"Fixed-size buffer to store transition tuples.\"\"\"\n",
        "\n",
        "  def __init__(self, buffer_capacity: int, alpha: float = 0.4, beta: float = 0.6 , epsilon: float = 1e-6 ):\n",
        "      \"\"\"Initialize a ReplayBuffer object.\n",
        "      Args:\n",
        "          batch_size (int): size of each training batch\n",
        "      \"\"\"\n",
        "      self._memory = list()\n",
        "      self._maxlen = buffer_capacity\n",
        "      self._alpha = alpha\n",
        "      self._beta = beta\n",
        "      self._epsilon = epsilon\n",
        "      self._index = 0\n",
        "      self._priorities = np.zeros(buffer_capacity, dtype=np.float32)\n",
        "\n",
        "  @property\n",
        "  def size(self) -> int:\n",
        "    # Return the current number of elements in the buffer.\n",
        "    return len(self._memory)\n",
        "\n",
        "  def add(self, state_t: chex.Array,\n",
        "          action_t: chex.Array,\n",
        "          reward_t: chex.Array,\n",
        "          done_t: chex.Array,\n",
        "          state_tp1: chex.Array,\n",
        "          priority: chex.Array) -> None:\n",
        "      \"\"\"Add a new transition to memory.\"\"\"\n",
        "\n",
        "      if self.size < self._maxlen:\n",
        "        self._memory.append(Transition(state_t=state_t,\n",
        "                                action_t = action_t,\n",
        "                                reward_t = reward_t,\n",
        "                                done_t = done_t,\n",
        "                                state_tp1 = state_tp1))\n",
        "      else:\n",
        "        self._memory[self._index] = Transition(state_t=state_t,\n",
        "                                action_t = action_t,\n",
        "                                reward_t = reward_t,\n",
        "                                done_t = done_t,\n",
        "                                state_tp1 = state_tp1)\n",
        "      self._priorities[self._index] = priority\n",
        "      self._index = (self._index + 1) % self._maxlen\n",
        "\n",
        "  def sample_batch(self, batch_size) -> Transition:\n",
        "      \"\"\"Randomly sample a transition from memory.\"\"\"\n",
        "      assert self._memory, 'replay buffer is unfilled'\n",
        "      assert len(self._memory) >= batch_size, 'Insuficient number of transitions in replay buffer'\n",
        "\n",
        "      priorities = self._priorities[:len(self._memory)]\n",
        "      probabilities = priorities ** self._alpha\n",
        "      probabilities /= probabilities.sum()\n",
        "      indices = np.random.choice(len(self._memory), batch_size, p=probabilities)\n",
        "      samples = [self._memory[i] for i in indices]\n",
        "      weights = (len(self._memory) * probabilities[indices]) ** (-self._beta)\n",
        "      weights /= weights.max()\n",
        "      weights = np.array(weights, dtype=np.float32)\n",
        "\n",
        "      kwargs = dict()\n",
        "      for attr in [\"state_t\", \"action_t\", \"reward_t\", \"done_t\", \"state_tp1\"]:\n",
        "          kwargs[attr] = np.array([getattr(s, attr) for s in samples])\n",
        "      return Transition(**kwargs), indices, weights\n",
        "    \n",
        "\n",
        "  def update_priorities(self, indices, priorities):\n",
        "      for i, p in zip(indices, priorities):\n",
        "          self._priorities[i] = abs(p) + self._epsilon"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Config"
      ],
      "metadata": {
        "id": "-K_doixqzpsY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_vZ3RK_bhITl"
      },
      "outputs": [],
      "source": [
        "@dataclass()\n",
        "class TrainingConfig:\n",
        "  num_episodes : int = 15000\n",
        "  num_eval_episodes : int = 20\n",
        "  eval_every_N : int = 10\n",
        "  gamma : float = 0.9\n",
        "  eps : float = 1.0\n",
        "  eps_decay_per_episode : float = 0.9995\n",
        "  min_eps : float = 0.1\n",
        "  learning_rate : float = 1e-3\n",
        "  buffer_capacity : int = 5000\n",
        "  min_buffer_capacity : int = 64\n",
        "  batch_size: int = 64\n",
        "  target_ema=0.75\n",
        "  seed : int = 0\n",
        "  max_steps : int = 500\n",
        "  update_every = 50\n",
        "\n",
        "  probability_optimal = 0.9\n",
        "  probability_optimal_decay = 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# !! Please run only the cells for the selected method (Deep Q-Learning, Reinforce or A2C)"
      ],
      "metadata": {
        "id": "4oIRB3Hd7Y0W"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQQDt7TKI4Ru"
      },
      "source": [
        "#Deep Q-Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZVDFft8QqXa"
      },
      "outputs": [],
      "source": [
        "@chex.dataclass\n",
        "class LearnerState:\n",
        "  online_params: hk.Params\n",
        "  target_params: hk.Params\n",
        "  opt_state: optax.OptState\n",
        "\n",
        "class DeepAgent:\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      env: Maze,\n",
        "      network : callable,\n",
        "      cfg : TrainingConfig,\n",
        "      ) -> None:\n",
        "    \"\"\"Initializes the DQN agent.\n",
        "\n",
        "    Args:\n",
        "      env: Maze.\n",
        "      gamma: discount factor\n",
        "      eps: probability to perform a random exploration when picking a new action.\n",
        "      learning_rate: learning rate of the online network\n",
        "      buffer_capacity: capacity of the replay buffer\n",
        "      min_buffer_capacity: min buffer size before picking batches from the\n",
        "        replay buffer to update the online network\n",
        "      batch_size: batch size when updating the online network\n",
        "      target_ema: weight when updating the target network.\n",
        "      seed: seed of the random generator.\n",
        "    \"\"\"\n",
        "    self._env = env\n",
        "    self._learning_rate = cfg.learning_rate\n",
        "    self._eps = cfg.eps\n",
        "    self._gamma = cfg.gamma\n",
        "    self._batch_size = cfg.batch_size\n",
        "    self._target_ema = cfg.target_ema\n",
        "    self._Na = env.num_actions()\n",
        "    self._network = network\n",
        "    self._steps = 0\n",
        "    self._update_every = cfg.update_every\n",
        "\n",
        "    # Define the neural network for this agent\n",
        "    self._init, self._apply = hk.without_apply_rng(hk.transform(self._hk_qfunction))\n",
        "    # Jit the forward pass of the neural network for better performances\n",
        "    self.apply = jax.jit(self._apply)\n",
        "\n",
        "    # Also jit the update functiom\n",
        "    self._update_fn = jax.jit(self._update_fn)\n",
        "\n",
        "    # Initialize the network's parameters\n",
        "    self._rng = jax.random.PRNGKey(cfg.seed)\n",
        "    self._rng, init_rng = jax.random.split(self._rng)\n",
        "    self._learner_state = self._init_state(init_rng)\n",
        "\n",
        "    # Initialize the replay buffer\n",
        "    self._min_buffer_capacity = cfg.min_buffer_capacity\n",
        "    #self._buffer = BatchedReplayBuffer(cfg.buffer_capacity)\n",
        "    self._buffer = BatchedReplayBuffer(cfg.buffer_capacity)\n",
        "\n",
        "    # Build a variable to store the last state observed by the agent\n",
        "    self._state = None\n",
        "\n",
        "    #best params\n",
        "    self._best_params = None\n",
        "\n",
        "  def _optimizer(self) -> optax.GradientTransformation:\n",
        "    return optax.adam(learning_rate=self._learning_rate)\n",
        "\n",
        "  def _hk_qfunction(self, state: chex.Array) -> chex.Array:\n",
        "    return self._network(state, self._env)\n",
        "\n",
        "  def first_observe(self, state: chex.Array) -> None:\n",
        "    self._state = state\n",
        "\n",
        "  def _init_state(self, rng: chex.PRNGKey) -> LearnerState:\n",
        "    \"\"\"Initialize the online parameters, the target parameters and the\n",
        "    optimizer's state.\"\"\"\n",
        "\n",
        "    #dummy step for the network to infer the input shape\n",
        "    dummy_step = self._env.reset().copy()[None]\n",
        "\n",
        "    #split the rng to have a different stochasticity between the target and the online parameters\n",
        "    online_rng, target_rng = jax.random.split(rng)\n",
        "\n",
        "    #initializing the parameters and the optimizer\n",
        "    online_params = self._init(online_rng, dummy_step)\n",
        "    target_params = self._init(target_rng, dummy_step)\n",
        "    opt_state = self._optimizer().init(online_params)\n",
        "\n",
        "\n",
        "    return LearnerState(\n",
        "        online_params = online_params,\n",
        "        target_params = target_params,\n",
        "        opt_state = opt_state,\n",
        "    )\n",
        "  \n",
        "  def act(self, state: chex.Array, eval: bool = False, explore: bool = False) -> int:\n",
        "    #if not in eval mode or explore mode we might do some exploration\n",
        "    if not eval and (explore or np.random.uniform() < self._eps):\n",
        "      return np.random.randint(self._Na)\n",
        "\n",
        "    #returning the greedy action\n",
        "    online_params = self._learner_state.online_params\n",
        "    return jnp.argmax(self.apply(online_params, state[None]), axis=-1).item()\n",
        "\n",
        "  def loss_fn(\n",
        "      self,\n",
        "      online_params: hk.Params,\n",
        "      target_params: hk.Params,\n",
        "      state_t: chex.Array,\n",
        "      action_t: chex.Array,\n",
        "      reward_t: chex.Array,\n",
        "      done_t: chex.Array,\n",
        "      state_tp1: chex.Array,\n",
        "      ) -> chex.Array:\n",
        "      \"\"\"Computes the Q-learning loss\n",
        "\n",
        "      Args:\n",
        "        online_params: parameters of the online network\n",
        "        target_params: parameters of the target network\n",
        "        state_t: batch of observations at time t\n",
        "        action_t: batch of actions performed at time t\n",
        "        reward_t: batch of rewards obtained at time t\n",
        "        done_t: batch of end of episode status at time t\n",
        "        state_tp1: batch of states at time t+1\n",
        "      Returns:\n",
        "        The Q-learning loss.\n",
        "      \"\"\"\n",
        "      \n",
        "      targets = reward_t + self._gamma * jnp.max((1 - done_t[..., None]) * self.apply(target_params, state_tp1), axis=1)\n",
        "      predictions = self.apply(online_params, state_t)\n",
        "      predictions = jax.vmap(lambda pred, action : pred[action])(predictions, action_t)\n",
        "      loss = jnp.mean(jnp.square(predictions-targets))\n",
        "      return loss\n",
        "  \n",
        "  def _update_fn(self,\n",
        "                 state: LearnerState,\n",
        "                 batch: Transition,\n",
        "                 ) -> Tuple[chex.Array, LearnerState]:\n",
        "    \"\"\"Get the next learner state given the current batch of transitions.\n",
        "\n",
        "    Args:\n",
        "      state: learner state before update.\n",
        "      batch: batch of experiences (st, at, rt, done_t, stp1)\n",
        "    Returns:\n",
        "      loss, learner state after update\n",
        "    \"\"\"\n",
        "\n",
        "    # Compute gradients\n",
        "    online_params = state.online_params\n",
        "    target_params = state.target_params\n",
        "    loss, grads = jax.value_and_grad(self.loss_fn)(online_params, \n",
        "                                                             target_params, \n",
        "                                                             batch.state_t, \n",
        "                                                             batch.action_t,\n",
        "                                                             batch.reward_t,\n",
        "                                                             batch.done_t,\n",
        "                                                             batch.state_tp1)\n",
        "\n",
        "    # Apply gradients\n",
        "    opt_state = state.opt_state\n",
        "    updates, opt_state = self._optimizer().update(grads, opt_state)\n",
        "    online_params = optax.apply_updates(online_params, updates)\n",
        "    \n",
        "\n",
        "    # Update target network params as:\n",
        "    # target_params <- ema * target_params + (1 - ema) * online_params\n",
        "    \n",
        "    target_params = jax.tree_map(lambda x, y : self._target_ema * x + (1 - self._target_ema) * y, target_params, online_params)\n",
        "    \n",
        "    next_state = LearnerState(online_params=online_params,\n",
        "                              target_params=target_params,\n",
        "                              opt_state=opt_state)\n",
        "    return loss, next_state\n",
        "  \n",
        "  def _temporal_differences(self,\n",
        "                  batch : Transition\n",
        "                  ) -> chex.Array:\n",
        "    \"\"\"We have a batch of transition on which we want to compute their temporal difference w.r.t.\n",
        "    the current online and target network\n",
        "    \"\"\"\n",
        "    \n",
        "    state_t = batch.state_t\n",
        "    action_t = batch.action_t\n",
        "    reward_t = batch.reward_t\n",
        "    done_t = batch.done_t\n",
        "    state_tp1 = batch.state_tp1\n",
        "\n",
        "    online_params = self._learner_state.online_params\n",
        "    target_params = self._learner_state.target_params\n",
        "    targets = reward_t + self._gamma * jnp.max((1 - done_t[..., None]) * self.apply(target_params, state_tp1), axis=1)\n",
        "    predictions = self.apply(online_params, state_t)\n",
        "    predictions = jax.vmap(lambda pred, action : pred[action])(predictions, action_t)\n",
        "\n",
        "    td = targets - predictions\n",
        "    return td\n",
        "      \n",
        "  def observe(self,\n",
        "              action_t: chex.Array,\n",
        "              reward_t: chex.Array,\n",
        "              done_t: chex.Array,\n",
        "              state_tp1: chex.Array,\n",
        "              ) -> chex.Array:\n",
        "    \"\"\"Updates the agent from the given observations.\n",
        "\n",
        "    Args:\n",
        "      action_t: action performed at time t.\n",
        "      reward_t: reward obtained after having performed action_t.\n",
        "      done_t: whether or not the episode is over after performing action_t.\n",
        "      state_tp1: state at which the environment is at time t+1.\n",
        "    Returns:\n",
        "      DQN loss obtained when updating the online network.\n",
        "    \"\"\"\n",
        "\n",
        "    #compute the priority associated with the transition. p = |td_error|\n",
        "\n",
        "    \"\"\"priority = self._priorities(Transition(state_t = self._state[None], \n",
        "                                           action_t = np.array([action_t]), \n",
        "                                           reward_t = np.array([reward_t]), \n",
        "                                           done_t = np.array([done_t]), \n",
        "                                           state_tp1 = state_tp1[None]))[0]\"\"\"\n",
        "    self._buffer.add(self._state, action_t, reward_t, done_t, state_tp1) #priority)\n",
        "    self._state = state_tp1\n",
        "\n",
        "    # We update the agent if and only if we have enough states stored in\n",
        "    # memory.\n",
        "    if self._buffer.size >= self._min_buffer_capacity and self._steps % self._update_every == 0:\n",
        "      batch = self._buffer.sample_batch(self._batch_size)\n",
        "      #priorities = self._priorities(batch)\n",
        "      #self._buffer.update_priorities(indices, priorities)\n",
        "      loss, self._learner_state = self._update_fn(self._learner_state, batch)\n",
        "      return loss\n",
        "    self._steps += 1\n",
        "    return None\n",
        "\n",
        "  def decay_epsilon(self) -> None:\n",
        "    self._eps = max(cfg.min_eps, cfg.eps_decay_per_episode * self._eps)\n",
        "    cfg.eps = self._eps\n",
        "  \n",
        "  def save_checkpoint(self) -> None:\n",
        "    self._best_params = self._learner_state.online_params"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reinforce\n"
      ],
      "metadata": {
        "id": "6wWae--BHrgs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cumulative_returns(rewards: chex.Array,\n",
        "                       dones: chex.Array,\n",
        "                       gamma: float,\n",
        "                       ) -> chex.Array:\n",
        "  returns = [0.]\n",
        "  belongs_to_an_unfinished_episode = [1.]\n",
        "  for r, d in zip(rewards[::-1], dones[::-1]):\n",
        "    returns.append(r + gamma * (1 - d) * returns[-1])\n",
        "    belongs_to_an_unfinished_episode.append((1 - d) * belongs_to_an_unfinished_episode[-1])\n",
        "  does_not_belong_to_an_unfinished_episode = 1 - jnp.stack(belongs_to_an_unfinished_episode)[::-1][:-1]\n",
        "  return jnp.stack(returns)[::-1][:-1] * does_not_belong_to_an_unfinished_episode"
      ],
      "metadata": {
        "id": "X0jrDvnfHuXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import *\n",
        "from collections import deque\n",
        "\n",
        "\n",
        "@chex.dataclass\n",
        "class LearnerState:\n",
        "  params: hk.Params\n",
        "  opt_state: optax.OptState\n",
        "\n",
        "class REINFORCEAgent:\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      env: Maze,\n",
        "      network : callable,\n",
        "      cfg : TrainingConfig,\n",
        "      ) -> None:\n",
        "\n",
        "    # Basic parameters\n",
        "    self._env = env\n",
        "    self._Na = env.num_actions()\n",
        "    self._learning_rate = cfg.learning_rate\n",
        "    self._gamma = cfg.gamma\n",
        "    self._network = network\n",
        "\n",
        "    # The agent is not updated at every step.\n",
        "    self._steps_between_updates = cfg.update_every\n",
        "    self._steps = 0\n",
        "\n",
        "    # Initialize the random generator\n",
        "    self._rng = jax.random.PRNGKey(cfg.seed)\n",
        "    self._rng, init_rng = jax.random.split(self._rng)\n",
        "\n",
        "    # Initialize the network function\n",
        "    self._init, self._apply = hk.without_apply_rng(hk.transform(self._hk_policy_function))\n",
        "    # Jit both the forward pass and the update function for more efficiency\n",
        "    self.apply = jax.jit(self._apply)\n",
        "    self._update_fn = jax.jit(self._update_fn)\n",
        "\n",
        "    # Initialize the parameters of the neural network as well as the optimizer\n",
        "    self._learner_state = self._init_state(init_rng)\n",
        "\n",
        "    # Small buffers to store trajectories\n",
        "    self._states = deque([], self._steps_between_updates + 1)\n",
        "    self._actions = deque([], self._steps_between_updates + 1)\n",
        "    self._rewards = deque([], self._steps_between_updates + 1)\n",
        "    self._dones = deque([], self._steps_between_updates + 1)\n",
        "\n",
        "    #save the parameters that performed the best at evaluation:\n",
        "\n",
        "    self._best_params = None\n",
        "\n",
        "  def _optimizer(self) -> optax.GradientTransformation:\n",
        "    return optax.adam(learning_rate=self._learning_rate)\n",
        "\n",
        "  def _hk_policy_function(self, state: chex.Array) -> chex.Array:\n",
        "    return self._network(state, self._env)\n",
        "\n",
        "  def observe(self,\n",
        "              state_t: chex.Array,\n",
        "              action_t: chex.Array,\n",
        "              reward_t: chex.Array,\n",
        "              done_t: chex.Array,\n",
        "              ) -> Optional[chex.Array]:\n",
        "    \"\"\"Observes the current transition and updates the network if necessary.\n",
        "\n",
        "    Args:\n",
        "      state_t: state observed at time t.\n",
        "      action_t: action performed at time t.\n",
        "      reward_t: reward obtained after performing  action_t.\n",
        "      done_t: wether or not the episode is over after the step.\n",
        "    Returns:\n",
        "      The training loss if the model was updated, None elsewhere.\n",
        "    \"\"\"\n",
        "    # /!\\ READ THIS PART CAREFULLY IN CONJUNCTION WITH THE TRAINING LOOP /!\\\n",
        "    # /!\\ IT IS IMPORTANT TO KEEP ALL THINGS IN THE SAME ORDER IN THE    /!\\\n",
        "    # /!\\ BUFFERS, AND TO NOT MESS UP THE INDICES (NOTABLY THE FIRST).   /!\\\n",
        "    self._states.append(state_t)\n",
        "    self._actions.append(action_t)\n",
        "    self._rewards.append(reward_t)\n",
        "    self._dones.append(done_t)\n",
        "    self._steps += 1\n",
        "\n",
        "    do_update = self._steps %  self._steps_between_updates\n",
        "    do_update = do_update and self._steps >= self._steps_between_updates + 1\n",
        "\n",
        "    if do_update:\n",
        "      states = np.stack(self._states, axis=0)\n",
        "      actions = np.stack(self._actions, axis=0)[:-1]\n",
        "      rewards = np.stack(self._rewards, axis=0)[:-1]\n",
        "      dones = np.stack(self._dones, axis=0)[:-1]\n",
        "\n",
        "      loss, self._learner_state = self._update_fn(self._learner_state,\n",
        "                                                  states,\n",
        "                                                  actions,\n",
        "                                                  rewards,\n",
        "                                                  dones)\n",
        "      return loss\n",
        "    return None\n",
        "\n",
        "  def _init_state(self, rng: chex.PRNGKey) -> LearnerState:\n",
        "    \"\"\"Initializes the parameter of the neural network and the optimizer.\"\"\"\n",
        "    # Your code here !\n",
        "    state = self._env.reset().copy()\n",
        "    bstate = state[None]\n",
        "    params = self._init(rng, bstate)\n",
        "    opt_state = self._optimizer().init(params)\n",
        "    return LearnerState(params=params, opt_state=opt_state)\n",
        "\n",
        "\n",
        "  def act(self,\n",
        "          state: chex.Array,\n",
        "          eval: bool,\n",
        "          explore : bool\n",
        "          ) -> chex.Array:\n",
        "    \"\"\"Pick the next action according to the learnt policy.\"\"\"\n",
        "    # Your code here\n",
        "    bstate = state[None]\n",
        "    logits = self.apply(self._learner_state.params, bstate)[0]\n",
        "    if eval:\n",
        "      return jnp.argmax(logits)\n",
        "    self._rng, rng = jax.random.split(self._rng)\n",
        "    if explore:\n",
        "      return jax.random.randint(rng, minval=0, maxval=self._Na, shape=())\n",
        "    return jax.random.categorical(rng, logits)\n",
        "\n",
        "  def loss_fn(\n",
        "      self,\n",
        "      params: hk.Params,\n",
        "      states: chex.Array,\n",
        "      actions: chex.Array,\n",
        "      rewards: chex.Array,\n",
        "      dones: chex.Array,\n",
        "      ) -> chex.Array:\n",
        "      \"\"\"Compute the loss function for REINFORCE.\n",
        "\n",
        "      Args:\n",
        "        params: network parameters.\n",
        "        states: a tensor of shape (T+1, N_rows, N_cols) representing the states\n",
        "          observed from time 0 to T. N_rows, and N_cols are respectively the\n",
        "          number of rows and columns in the catch environment.\n",
        "        actions: a tensor of shape (T,) giving the actions performed from time\n",
        "          0 to T-1.\n",
        "        rewards: a tensor of shape (T,) giving the rewards obtained from time\n",
        "          0 to T-1.\n",
        "        dones: a tensor of shape (T,) giving the 'end of episode status' from\n",
        "        time 0 to T-1.\n",
        "      Returns:\n",
        "        training_loss\n",
        "      \"\"\"\n",
        "      # Your code here !\n",
        "      log_probs = jax.nn.log_softmax(self.apply(params, states), axis=-1) # (T + 1, A)\n",
        "      log_probs_actions = jax.vmap(lambda l, a: l[a])(log_probs[:-1], actions) # (T,)\n",
        "\n",
        "      cs = cumulative_returns(rewards, dones, self._gamma) # (T,)\n",
        "\n",
        "      return -jnp.mean(log_probs_actions * cs)\n",
        "\n",
        "  def _update_fn(self,\n",
        "                 learner_state: LearnerState,\n",
        "                 states: chex.Array,\n",
        "                 actions: chex.Array,\n",
        "                 rewards: chex.Array,\n",
        "                 dones: chex.Array,\n",
        "                 ) -> Tuple[chex.Array, LearnerState]:\n",
        "    \"\"\"Updates the network.\n",
        "\n",
        "    Args:\n",
        "      learner_state: network and optimizer parameters.\n",
        "      states: a tensor of shape (T+1, N_rows, N_cols) representing the states\n",
        "        observed from time 0 to T. N_rows, and N_cols are respectively the\n",
        "        number of rows and columns in the catch environment.\n",
        "      actions: a tensor of shape (T,) giving the actions performed from time\n",
        "        0 to T-1.\n",
        "      rewards: a tensor of shape (T,) giving the rewards obtained from time\n",
        "        0 to T-1.\n",
        "      dones: a tensor of shape (T,) giving the 'end of episode status' from time\n",
        "        0 to T-1.\n",
        "    Returns:\n",
        "      training_loss, next network ann optimizer's parameters\n",
        "    \"\"\"\n",
        "    # Your code here !\n",
        "    loss, grad = jax.value_and_grad(self.loss_fn)(learner_state.params, states, actions, rewards, dones)\n",
        "    updates, new_opt_state = self._optimizer().update(grad, learner_state.opt_state)\n",
        "    new_params = optax.apply_updates(learner_state.params, updates)\n",
        "\n",
        "    return loss, LearnerState(params=new_params, opt_state=new_opt_state)\n",
        "\n",
        "  def save_checkpoint(self) -> None:\n",
        "    self._best_params = self._learner_state.params"
      ],
      "metadata": {
        "id": "Dqoa7zXbIS6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A2C"
      ],
      "metadata": {
        "id": "qgkq1IKnIO6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def one_step_temporal_difference(values: chex.Array,\n",
        "                                 rewards: chex.Array,\n",
        "                                 dones: chex.Array,\n",
        "                                 gamma: float,\n",
        "                                 ) -> chex.Array:\n",
        "  return rewards + gamma * jax.lax.stop_gradient(values[1:]*(1 - dones)) - values[:-1]"
      ],
      "metadata": {
        "id": "9NkjAKY4IRWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import *\n",
        "from collections import deque\n",
        "\n",
        "@chex.dataclass\n",
        "class LearnerState:\n",
        "  policy_params: hk.Params\n",
        "  value_params: hk.Params\n",
        "  policy_opt_state: optax.OptState\n",
        "  value_opt_state: optax.OptState\n",
        "\n",
        "class A2CAgent:\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      env: Maze,\n",
        "      policy_network : callable,\n",
        "      value_network : callable,\n",
        "      cfg : TrainingConfig,\n",
        "      ) -> None:\n",
        "\n",
        "    self._env = env\n",
        "    self._learning_rate = cfg.learning_rate\n",
        "    self._gamma = cfg.gamma\n",
        "    self._Na = env.num_actions()\n",
        "    self._policy_network = policy_network\n",
        "    self._value_network = value_network\n",
        "\n",
        "    # Do not update at each step\n",
        "    self._steps_between_updates = cfg.update_every\n",
        "    self._steps = 0\n",
        "\n",
        "    # Random generator\n",
        "    self._rng = jax.random.PRNGKey(cfg.seed)\n",
        "    self._rng, init_rng = jax.random.split(self._rng)\n",
        "\n",
        "    # Build the policy and value networks\n",
        "    self._policy_init, self._policy_apply = hk.without_apply_rng(hk.transform(self._hk_policy_function))\n",
        "    self._value_init, self._value_apply = hk.without_apply_rng(hk.transform(self._hk_value_function))\n",
        "\n",
        "    # Jit the forward and update functions for more efficiency\n",
        "    self.policy_apply = jax.jit(self._policy_apply)\n",
        "    self.value_apply = jax.jit(self._value_apply)\n",
        "    self._update_fn = jax.jit(self._update_fn)\n",
        "\n",
        "    # Initialize the networks and optimizer's parameters\n",
        "    self._learner_state = self._init_state(init_rng)\n",
        "\n",
        "    # Small buffers to store trajectories\n",
        "    self._states = deque([], self._steps_between_updates + 1)\n",
        "    self._actions = deque([], self._steps_between_updates + 1)\n",
        "    self._rewards = deque([], self._steps_between_updates + 1)\n",
        "    self._dones = deque([], self._steps_between_updates + 1)\n",
        "\n",
        "    #best params\n",
        "    self._best_params = None\n",
        "\n",
        "  def _hk_policy_function(self, state: chex.Array) -> chex.Array:\n",
        "    return self._policy_network(state, self._env)\n",
        "\n",
        "  def _hk_value_function(self, state: chex.Array) -> chex.Array:\n",
        "    return self._value_network(state, self._env)\n",
        "\n",
        "  def _optimizer(self) -> optax.GradientTransformation:\n",
        "    return optax.adam(learning_rate=self._learning_rate)\n",
        "\n",
        "  def observe(self,\n",
        "              state_t: chex.Array,\n",
        "              action_t: chex.Array,\n",
        "              reward_t: chex.Array,\n",
        "              done_t: chex.Array,\n",
        "              ) -> Optional[chex.Array]:\n",
        "\n",
        "\n",
        "    # /!\\ READ THIS PART CAREFULLY IN CONJUNCTION WITH THE TRAINING LOOP /!\\\n",
        "    # /!\\ IT IS IMPORTANT TO KEEP ALL THINGS IN THE SAME ORDER IN THE    /!\\\n",
        "    # /!\\ BUFFERS, AND TO NOT MESS UP THE INDICES (NOTABLY THE FIRST).   /!\\\n",
        "    self._states.append(state_t)\n",
        "    self._actions.append(action_t)\n",
        "    self._rewards.append(reward_t)\n",
        "    self._dones.append(done_t)\n",
        "    self._steps += 1\n",
        "\n",
        "    if self._steps %  self._steps_between_updates and self._steps >= self._steps_between_updates + 1:\n",
        "      states = np.stack(self._states, axis=0)\n",
        "      actions = np.stack(self._actions, axis=0)[:-1]\n",
        "      rewards = np.stack(self._rewards, axis=0)[:-1]\n",
        "      dones = np.stack(self._dones, axis=0)[:-1]\n",
        "\n",
        "      loss, self._learner_state = self._update_fn(self._learner_state,\n",
        "                                                  states,\n",
        "                                                  actions,\n",
        "                                                  rewards,\n",
        "                                                  dones)\n",
        "      return loss\n",
        "    return None\n",
        "\n",
        "  def _init_state(self, rng: chex.PRNGKey) -> LearnerState:\n",
        "    \"\"\"Initializea the parameters of the policy network, the value network and\n",
        "    the optimizer.\n",
        "    \"\"\"\n",
        "    # Your code here\n",
        "    state = self._env.reset().copy()\n",
        "    bstate = state[None]\n",
        "    policy_params = self._policy_init(rng, bstate)\n",
        "    value_params = self._value_init(rng, bstate)\n",
        "    policy_opt_state = self._optimizer().init(policy_params)\n",
        "    value_opt_state = self._optimizer().init(value_params)\n",
        "    return LearnerState(policy_params=policy_params, policy_opt_state=policy_opt_state, value_params=value_params, value_opt_state=value_opt_state)\n",
        "\n",
        "\n",
        "  def act(self,\n",
        "          state: chex.Array,\n",
        "          eval: bool,\n",
        "          explore: bool\n",
        "          ) -> chex.Array:\n",
        "    \"\"\"Pick the next action according to the learnt poslicy.\"\"\"\n",
        "    # Your code here\n",
        "    bstate = state[None]\n",
        "    logits = self.policy_apply(self._learner_state.policy_params, bstate)[0]\n",
        "    if eval:\n",
        "      return jnp.argmax(logits)\n",
        "    self._rng, rng = jax.random.split(self._rng)\n",
        "    if explore:\n",
        "      return jax.random.randint(rng, minval=0, maxval=self._Na, shape=())\n",
        "    return jax.random.categorical(rng, logits)\n",
        "\n",
        "  def losses_fn(self,\n",
        "                policy_params: hk.Params,\n",
        "                value_params: hk.Params,\n",
        "                states: chex.Array,\n",
        "                actions: chex.Array,\n",
        "                rewards: chex.Array,\n",
        "                dones: chex.Array,\n",
        "                ) -> Tuple[chex.Array, chex.Array]:\n",
        "    \"\"\"Compute the policy and value losses.\n",
        "\n",
        "    Args:\n",
        "      policy_params: parameters of the policy network.\n",
        "      value_params: parameters of the value network.\n",
        "      states: a tensor of shape (T+1, N_rows, N_cols) representing the states\n",
        "        observed from time 0 to T. N_rows, and N_cols are respectively the\n",
        "        number of rows and columns in the catch environment.\n",
        "      actions: a tensor of shape (T,) giving the actions performed from time\n",
        "        0 to T-1.\n",
        "      rewards: a tensor of shape (T,) giving the rewards obtained from time\n",
        "        0 to T-1.\n",
        "      dones: a tensor of shape (T,) giving the 'end of episode status' from time\n",
        "        0 to T-1.\n",
        "    Returns:\n",
        "      policy_loss, value_loss\n",
        "    \"\"\"\n",
        "    # Your code here !\n",
        "    log_probs = jax.nn.log_softmax(self._policy_apply(policy_params, states), axis=-1) # (T + 1, A)\n",
        "    log_probs_actions = jax.vmap(lambda l, a: l[a])(log_probs[:-1], actions) # (T,)\n",
        "\n",
        "    values = self._value_apply(value_params, states)\n",
        "\n",
        "    advantages = one_step_temporal_difference(values, rewards, dones, self._gamma)\n",
        "    value_loss = jnp.mean(advantages ** 2)\n",
        "    policy_loss = -jnp.mean(log_probs_actions * advantages)\n",
        "\n",
        "    return policy_loss, value_loss\n",
        "\n",
        "  def _update_fn(self,\n",
        "                 learner_state: LearnerState,\n",
        "                 states: chex.Array,\n",
        "                 actions: chex.Array,\n",
        "                 rewards: chex.Array,\n",
        "                 dones: chex.Array,\n",
        "                 ) -> Tuple[chex.Array, LearnerState]:\n",
        "    \"\"\"Update the policy and the value networks.\n",
        "\n",
        "    Args:\n",
        "      learner_state: networks and optimizer parameters.\n",
        "      states: a tensor of shape (T+1, N_rows, N_cols) representing the states\n",
        "        observed from time 0 to T. N_rows, and N_cols are respectively the\n",
        "        number of rows and columns in the catch environment.\n",
        "      actions: a tensor of shape (T,) giving the actions performed from time\n",
        "        0 to T-1.\n",
        "      rewards: a tensor of shape (T,) giving the rewards obtained from time\n",
        "        0 to T-1.\n",
        "      dones: a tensor of shape (T,) giving the 'end of episode status' from time\n",
        "        0 to T-1.\n",
        "    Returns:\n",
        "      training_loss, next network ann optimizer's parameters\n",
        "    \"\"\"\n",
        "    # Your code here !\n",
        "    def _policy_loss(*args, **kwargs):\n",
        "      return self.losses_fn(*args, **kwargs)[0]\n",
        "\n",
        "    def _value_loss(*args, **kwargs):\n",
        "      return self.losses_fn(*args, **kwargs)[1]\n",
        "\n",
        "    args = [learner_state.policy_params, learner_state.value_params, states, actions, rewards, dones]\n",
        "\n",
        "    policy_loss, policy_grad = jax.value_and_grad(_policy_loss)(*args)\n",
        "    value_loss, value_grad = jax.value_and_grad(_value_loss, argnums=1)(*args)\n",
        "    policy_updates, new_policy_opt_state = self._optimizer().update(policy_grad, learner_state.policy_opt_state)\n",
        "    value_updates, new_value_opt_state = self._optimizer().update(value_grad, learner_state.value_opt_state)\n",
        "\n",
        "    new_policy_params = optax.apply_updates(learner_state.policy_params, policy_updates)\n",
        "    new_value_params = optax.apply_updates(learner_state.value_params, value_updates)\n",
        "\n",
        "    return policy_loss + value_loss, LearnerState(policy_params=new_policy_params, policy_opt_state=new_policy_opt_state, value_params=new_value_params, value_opt_state=new_value_opt_state)\n",
        "\n",
        "  def save_checkpoint(self) -> None:\n",
        "    self._best_params = self._learner_state.policy_params"
      ],
      "metadata": {
        "id": "poWYGQRVZcTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training functions"
      ],
      "metadata": {
        "id": "GvuwWP3PI-PY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_dqn_episode(deep_agent: REINFORCEAgent, env: Maze, eval: bool, max_steps: int = 200, early_stopping : int = 10, visit_limit : int = 3, optimal : bool = False, render = False, ) -> Tuple[float, list]:\n",
        "  # Reset any counts and start the environment.\n",
        "  state = env.reset().copy()\n",
        "  #deep_agent.first_observe(state)\n",
        "  num_steps = 0\n",
        "\n",
        "  \n",
        "  optimal_actions, next = bfs(state)\n",
        "\n",
        "  #we don't know how many losses will be computed so we return a list\n",
        "  losses = []\n",
        "\n",
        "  #keeping tracks of all the cells that have been visited for early stoppping\n",
        "  visits = {}\n",
        "  x, y = np.nonzero(state[..., CellType.PLAYER.value - 1])\n",
        "  x, y = x[0], y[0]\n",
        "  visits[(x, y)] = 1\n",
        "\n",
        "  # Run an episode.\n",
        "  while True:\n",
        "    if render:\n",
        "      print(env.render())\n",
        "      print('------')\n",
        "    # Generate an action from the agent's policy and step the environment.\n",
        "\n",
        "    #if the cell has already been visited more than the limit in the same episode, then we force exploration\n",
        "    explore = (visits[(x, y)] >= visit_limit)\n",
        "\n",
        "    if optimal: #If chosen, we guide the model with the optimal path\n",
        "      action = optimal_actions[x][y]\n",
        "    else: #Otherwise we let it act by itself\n",
        "      action = deep_agent.act(state, eval, explore=explore)\n",
        "    next_state, reward, done = env.step(action)\n",
        "\n",
        "    #we keep track of the number of time a celle has been visited to force exploration if visited too frequently, cf higher\n",
        "    next_x, next_y = next[x][y][action]\n",
        "\n",
        "    if (next_x, next_y) in visits:\n",
        "      visits[(next_x, next_y)] += 1\n",
        "    else:\n",
        "      visits[(next_x, next_y)] = 1\n",
        "    \n",
        "    #set a negative reward to avoid the agent going into the walls\n",
        "    if next_x == x and next_y == y:\n",
        "        reward = -0.1\n",
        "    \n",
        "    if not eval:\n",
        "      loss = deep_agent.observe(state.copy(), action, reward, done)\n",
        "      #only append loss if it was really computed\n",
        "      if loss is not None:\n",
        "        losses.append(loss)\n",
        "    num_steps += 1\n",
        "    state = next_state.copy()\n",
        "    x, y = next_x, next_y\n",
        "    if done or num_steps > max_steps or visits[(x, y)] >= early_stopping:\n",
        "      break\n",
        "  if render:\n",
        "      print(env.render())\n",
        "  return reward, losses"
      ],
      "metadata": {
        "id": "6qJpZQQR9aFI",
        "execution": {
          "iopub.status.busy": "2023-03-04T20:07:57.187493Z",
          "iopub.execute_input": "2023-03-04T20:07:57.187883Z",
          "iopub.status.idle": "2023-03-04T20:07:57.201507Z",
          "shell.execute_reply.started": "2023-03-04T20:07:57.187850Z",
          "shell.execute_reply": "2023-03-04T20:07:57.200000Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(deep_agent : REINFORCEAgent, env : Maze, cfg : TrainingConfig, tracking=False, run_name : str = \"\") -> Tuple[list, list, list]:\n",
        "\n",
        "  if tracking:\n",
        "    # start a new wandb run to track this training session\n",
        "    wandb.init(\n",
        "        project=project_name,\n",
        "        \n",
        "        # track hyperparameters and run metadata\n",
        "        config = asdict(cfg),\n",
        "        name = run_name\n",
        "\n",
        "    )      \n",
        "\n",
        "  print(f\"Episode number:\\t| Average reward on {cfg.num_eval_episodes} eval episodes \\t| Mean training loss for the {cfg.eval_every_N} past episodes\")\n",
        "  print(\"------------------------------------------------------------------------------------------------------------\")\n",
        "\n",
        "  episodes = []\n",
        "  all_rewards = []\n",
        "  all_training_losses = []\n",
        "  current_training_losses = []\n",
        "\n",
        "  max_reward = -2\n",
        "\n",
        "  for episode in range(cfg.num_episodes):\n",
        "\n",
        "    #decays epsilon exploration\n",
        "    #deep_agent.decay_epsilon()\n",
        "\n",
        "    #decays probability_optimal \n",
        "    cfg.probability_optimal = cfg.probability_optimal * cfg.probability_optimal_decay\n",
        "\n",
        "    optimal = True if np.random.uniform() < cfg.probability_optimal else False\n",
        "    _, training_losses = run_dqn_episode(deep_agent, env, eval=False, max_steps = cfg.max_steps, optimal=optimal)\n",
        "    current_training_losses.extend(training_losses)\n",
        "\n",
        "    if episode % cfg.eval_every_N == 0:\n",
        "\n",
        "      reward = np.mean([run_dqn_episode(deep_agent, env, eval=True, max_steps = cfg.max_steps)[0] for _ in range(cfg.num_eval_episodes)])\n",
        "      mean_training_loss = np.mean(current_training_losses)\n",
        "\n",
        "      print(f\"\\t{episode}\\t|\\t{reward}\\t|{mean_training_loss}\")\n",
        "\n",
        "      all_rewards.append(reward)\n",
        "      episodes.append(episode)\n",
        "      all_training_losses.extend(training_losses)\n",
        "      current_training_losses = []\n",
        "\n",
        "      if tracking:\n",
        "        #upload logs\n",
        "        wandb.log({\"reward\": reward, \"Mean training loss\": mean_training_loss, \"Epsilon\" : cfg.eps, \"Max_steps\": cfg.max_steps, \"Prob Opt\": cfg.probability_optimal})\n",
        "\n",
        "      if reward > max_reward:\n",
        "        max_reward = reward\n",
        "        deep_agent.save_checkpoint()\n",
        "  if tracking:\n",
        "    #finish the session\n",
        "    wandb.finish()\n",
        "\n",
        "  return episodes, all_rewards, all_training_losses\n",
        "  "
      ],
      "metadata": {
        "id": "zs5ARHXrKo3w",
        "execution": {
          "iopub.status.busy": "2023-03-04T20:08:02.298598Z",
          "iopub.execute_input": "2023-03-04T20:08:02.299231Z",
          "iopub.status.idle": "2023-03-04T20:08:02.313501Z",
          "shell.execute_reply.started": "2023-03-04T20:08:02.299193Z",
          "shell.execute_reply": "2023-03-04T20:08:02.312190Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWtjeC51IxcW"
      },
      "source": [
        "# Model architectures"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def maze_value_CNN(x: chex.Array, env: Maze):\n",
        "  x = x.astype(float)\n",
        "  model = hk.Sequential([\n",
        "      hk.Conv2D(32, kernel_shape=[3,3], stride=2, padding='SAME'),\n",
        "      jax.nn.relu,\n",
        "      #hk.MaxPool(32, strides=2, padding='SAME'),\n",
        "      hk.Conv2D(64, kernel_shape=[2,2], stride=1, padding='SAME'),\n",
        "      jax.nn.relu,\n",
        "      #hk.MaxPool(64, strides=1, padding='SAME'),\n",
        "      hk.Flatten(),\n",
        "      hk.Linear(1)\n",
        "  ])\n",
        "  return model(x) "
      ],
      "metadata": {
        "id": "yunK3zGWy5m3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def maze_policy(x : chex.Array, env: Maze):\n",
        "    n_actions = env.num_actions()\n",
        "    x = x.astype(float)\n",
        "    model = hk.Sequential([\n",
        "        hk.Conv3D(32, kernel_shape=[6,6,5], stride=1),\n",
        "        jax.nn.relu,\n",
        "        hk.Conv3D(32, kernel_shape=[3,3,5], stride=1),\n",
        "        jax.nn.relu,\n",
        "        hk.Flatten(),\n",
        "        hk.Linear(64),\n",
        "        jax.nn.relu,\n",
        "        hk.Linear(4),\n",
        "        jax.nn.softmax\n",
        "    ])\n",
        "    return model(x)"
      ],
      "metadata": {
        "id": "QxIDQyWP0tF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def maze_network_CNN(x: chex.Array, env: Maze):\n",
        "  n_actions = env.num_actions()\n",
        "  x = x.astype(float)\n",
        "  model = hk.Sequential([\n",
        "      hk.Conv2D(32, kernel_shape=[3,3], stride=2, padding='SAME'),\n",
        "      jax.nn.relu,\n",
        "      #hk.MaxPool(32, strides=2, padding='SAME'),\n",
        "      hk.Conv2D(64, kernel_shape=[2,2], stride=1, padding='SAME'),\n",
        "      jax.nn.relu,\n",
        "      #hk.MaxPool(64, strides=1, padding='SAME'),\n",
        "      hk.Flatten(),\n",
        "      hk.Linear(n_actions)\n",
        "  ])\n",
        "  return model(x)"
      ],
      "metadata": {
        "id": "6a9oEE9ZwslZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def simple_linear(x: chex.Array, env: Maze):\n",
        "  n_actions = env.num_actions()\n",
        "  x = x.astype(float)\n",
        "  model = hk.Sequential([\n",
        "      hk.Flatten(),\n",
        "      hk.Linear(256),\n",
        "      jax.nn.relu,\n",
        "      hk.Linear(n_actions)\n",
        "  ])\n",
        "  return model(x)"
      ],
      "metadata": {
        "id": "bAIymOu2wrwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def q_network(x: chex.Array, env: Maze):\n",
        "  n_actions = env.num_actions()\n",
        "  x = x.astype(float)\n",
        "  conv = hk.Sequential([\n",
        "        hk.Conv2D(32, kernel_shape=(3, 3), stride=(2, 2), padding='VALID'),\n",
        "        jax.nn.relu,\n",
        "        hk.Conv2D(64, kernel_shape=(3, 3), stride=(2, 2), padding='VALID'),\n",
        "        jax.nn.relu,\n",
        "        hk.Conv2D(128, kernel_shape=(3, 3), stride=(2, 2), padding='VALID'),\n",
        "        jax.nn.relu\n",
        "    ])\n",
        "\n",
        "  linear = hk.Sequential([\n",
        "        hk.Flatten(),\n",
        "        hk.Linear(256),\n",
        "        jax.nn.relu,\n",
        "        hk.Linear(n_actions)\n",
        "    ])\n",
        "\n",
        "  x = conv(x)\n",
        "  x = linear(x)\n",
        "\n",
        "  return x\n"
      ],
      "metadata": {
        "id": "OFj9L9HmwpXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training the model"
      ],
      "metadata": {
        "id": "ms_5TkP98BdJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#For Deep Q-Learning\n",
        "cfg = TrainingConfig()\n",
        "env = Maze(BIG_MAZE)\n",
        "deep_agent = DeepAgent(\n",
        "    env=Maze(BIG_MAZE),\n",
        "    network = maze_network_CNN,\n",
        "    cfg=cfg\n",
        ")"
      ],
      "metadata": {
        "id": "kc-x9h4C8BR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4m7X438fZ_kf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4afdfaaa-9e50-4dd9-e00b-69a0a139ebc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/haiku/_src/base.py:515: UserWarning: Explicitly requested dtype float64 requested in zeros is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
            "  param = init(shape, dtype)\n"
          ]
        }
      ],
      "source": [
        "#For REINFORCE\n",
        "cfg = TrainingConfig()\n",
        "env = Maze(BIG_MAZE)\n",
        "deep_agent = REINFORCEAgent(\n",
        "    env=Maze(BIG_MAZE),\n",
        "    network = maze_network_CNN,\n",
        "    cfg=cfg\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#For A2C\n",
        "cfg = TrainingConfig()\n",
        "env = Maze(BIG_MAZE)\n",
        "deep_agent = A2CAgent(\n",
        "    env=Maze(BIG_MAZE),\n",
        "    policy_network = maze_network_CNN,\n",
        "    value_network = maze_value_CNN,\n",
        "    cfg=cfg\n",
        ")"
      ],
      "metadata": {
        "id": "67rrUxyz1kR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "episodes, all_rewards, all_training_losses = train(deep_agent, env, cfg, tracking=False, run_name = \"REINFORCE_CNN+LM+NR\")\n",
        "\n",
        "#plotting the results\n",
        "plt.xlabel('Number of training episodes')\n",
        "plt.ylabel('Average return')\n",
        "plt.plot(episodes, all_rewards)\n",
        "\n",
        "plt.figure()\n",
        "plt.xlabel('Number of training episodes')\n",
        "plt.ylabel('Mean training loss')\n",
        "plt.plot(all_training_losses)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WOiI1NDJzd2g",
        "outputId": "af61d917-7e8f-4253-e0d5-717019df56e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode number:\t| Average reward on 20 eval episodes \t| Mean training loss for the 10 past episodes\n",
            "------------------------------------------------------------------------------------------------------------\n",
            "\t0\t|\t0.085\t|0.0002227960794698447\n",
            "\t10\t|\t0.06499999999999999\t|-0.001709352363832295\n",
            "\t20\t|\t0.15999999999999998\t|0.25586605072021484\n",
            "\t30\t|\t0.12\t|0.11474012583494186\n",
            "\t40\t|\t0.16999999999999998\t|-0.17009280622005463\n",
            "\t50\t|\t0.09\t|0.11907302588224411\n",
            "\t60\t|\t0.165\t|0.12292755395174026\n",
            "\t70\t|\t0.145\t|0.10999687016010284\n",
            "\t80\t|\t0.125\t|-1.4926350116729736\n",
            "\t90\t|\t0.034999999999999996\t|443.857421875\n",
            "\t100\t|\t0.010000000000000004\t|-3.245047092437744\n",
            "\t110\t|\t-0.020000000000000007\t|1.395617961883545\n",
            "\t120\t|\t-0.045\t|1.8722482919692993\n",
            "\t130\t|\t0.14999999999999997\t|-0.15496857464313507\n",
            "\t140\t|\t-0.09000000000000002\t|4.219342231750488\n",
            "\t150\t|\t0.19999999999999998\t|0.08678529411554337\n",
            "\t160\t|\t-0.02000000000000001\t|0.029602844268083572\n",
            "\t170\t|\t0.06999999999999999\t|0.050830498337745667\n",
            "\t180\t|\t-2.7755575615628915e-18\t|0.4606497585773468\n",
            "\t190\t|\t-0.045\t|-1.2153191566467285\n",
            "\t200\t|\t0.08\t|0.04766404256224632\n",
            "\t210\t|\t-0.06500000000000002\t|138.59860229492188\n",
            "\t220\t|\t0.195\t|1.6824296712875366\n",
            "\t230\t|\t0.06499999999999999\t|-0.20666413009166718\n",
            "\t240\t|\t0.095\t|-0.5076742768287659\n",
            "\t250\t|\t0.185\t|0.1578616201877594\n",
            "\t260\t|\t0.025\t|0.18758530914783478\n",
            "\t270\t|\t0.075\t|-0.18915367126464844\n",
            "\t280\t|\t0.10499999999999998\t|0.05587568134069443\n",
            "\t290\t|\t0.09\t|-0.05217418819665909\n",
            "\t300\t|\t0.07999999999999999\t|0.022852368652820587\n",
            "\t310\t|\t-0.005000000000000002\t|0.7114300727844238\n",
            "\t320\t|\t0.225\t|-8.43997573852539\n",
            "\t330\t|\t0.02\t|-0.9133471846580505\n",
            "\t340\t|\t0.18\t|0.773001492023468\n",
            "\t350\t|\t0.18499999999999997\t|-0.6448525786399841\n",
            "\t360\t|\t0.29500000000000004\t|0.8182715177536011\n",
            "\t370\t|\t0.12999999999999998\t|0.2980929911136627\n",
            "\t380\t|\t0.10999999999999999\t|-0.8436912298202515\n",
            "\t390\t|\t0.06499999999999999\t|-2.6968510150909424\n",
            "\t400\t|\t0.43\t|-0.44723907113075256\n",
            "\t410\t|\t0.18\t|0.10712070763111115\n",
            "\t420\t|\t0.029999999999999992\t|-1.1680917739868164\n",
            "\t430\t|\t0.16999999999999998\t|1.3353382349014282\n",
            "\t440\t|\t0.06\t|0.604927659034729\n",
            "\t450\t|\t0.1\t|0.1607261598110199\n",
            "\t460\t|\t0.275\t|-0.18612012267112732\n",
            "\t470\t|\t0.19\t|-0.1402985006570816\n",
            "\t480\t|\t0.325\t|0.0018390074837952852\n",
            "\t490\t|\t0.135\t|0.2644096910953522\n",
            "\t500\t|\t-0.04\t|64.77845001220703\n",
            "\t510\t|\t0.06499999999999999\t|0.7800290584564209\n",
            "\t520\t|\t0.02\t|0.14414197206497192\n",
            "\t530\t|\t0.22999999999999998\t|-4.982384204864502\n",
            "\t540\t|\t0.16999999999999998\t|-0.3648012578487396\n",
            "\t550\t|\t0.095\t|-2.5182740688323975\n",
            "\t560\t|\t0.04\t|-1.246475338935852\n",
            "\t570\t|\t-0.03\t|-0.13740478456020355\n",
            "\t580\t|\t0.025000000000000005\t|-2.233149290084839\n",
            "\t590\t|\t0.275\t|-15.135750770568848\n",
            "\t600\t|\t0.13\t|-2.0167036056518555\n",
            "\t610\t|\t0.375\t|-1.7299257516860962\n",
            "\t620\t|\t0.23500000000000004\t|-1.3197389841079712\n",
            "\t630\t|\t0.32999999999999996\t|0.8637360334396362\n",
            "\t640\t|\t0.11499999999999999\t|0.30328378081321716\n",
            "\t650\t|\t0.16999999999999998\t|0.10025094449520111\n",
            "\t660\t|\t-0.015000000000000008\t|7.0451202392578125\n",
            "\t670\t|\t0.095\t|-34.16152572631836\n",
            "\t680\t|\t-0.035\t|5.4471330642700195\n",
            "\t690\t|\t0.15\t|1.6378483772277832\n",
            "\t700\t|\t0.04\t|13.776779174804688\n",
            "\t710\t|\t0.18\t|0.3639027774333954\n",
            "\t720\t|\t0.135\t|-1.753487229347229\n",
            "\t730\t|\t0.04\t|-23.164569854736328\n",
            "\t740\t|\t-0.020000000000000007\t|-33.10030746459961\n",
            "\t750\t|\t0.125\t|3.6830108165740967\n",
            "\t760\t|\t0.13\t|1.6207565069198608\n",
            "\t770\t|\t0.015000000000000003\t|-11.369983673095703\n",
            "\t780\t|\t0.15\t|0.23984920978546143\n",
            "\t790\t|\t0.0\t|6.466404914855957\n",
            "\t800\t|\t0.025\t|0.8933113217353821\n",
            "\t810\t|\t0.15\t|0.5639499425888062\n",
            "\t820\t|\t0.13\t|-1.103975772857666\n",
            "\t830\t|\t-4.163336342344337e-18\t|0.7229465246200562\n",
            "\t840\t|\t-0.02\t|-13.294647216796875\n",
            "\t850\t|\t-0.009999999999999997\t|140.27584838867188\n",
            "\t860\t|\t0.05\t|33.06544494628906\n",
            "\t870\t|\t0.024999999999999998\t|1.743222713470459\n",
            "\t880\t|\t0.21500000000000002\t|-19.92609214782715\n",
            "\t890\t|\t0.075\t|0.6145848631858826\n",
            "\t900\t|\t0.33\t|0.907400906085968\n",
            "\t910\t|\t0.02\t|-1.6791177988052368\n",
            "\t920\t|\t0.13999999999999999\t|-4.865826606750488\n",
            "\t930\t|\t0.21500000000000002\t|1.44064199924469\n",
            "\t940\t|\t0.43\t|0.6554536819458008\n",
            "\t950\t|\t0.004999999999999996\t|-14.145851135253906\n",
            "\t960\t|\t0.39\t|5.001143932342529\n",
            "\t970\t|\t0.06499999999999999\t|0.14539369940757751\n",
            "\t980\t|\t0.10999999999999999\t|-98.2044906616211\n",
            "\t990\t|\t0.32\t|0.9931803941726685\n",
            "\t1000\t|\t0.075\t|-132.1021270751953\n",
            "\t1010\t|\t0.095\t|6.242829322814941\n",
            "\t1020\t|\t-0.10000000000000002\t|102.92569732666016\n",
            "\t1030\t|\t-0.03\t|223.0350341796875\n",
            "\t1040\t|\t-0.015000000000000003\t|24.831710815429688\n",
            "\t1050\t|\t-0.025000000000000005\t|-26.014137268066406\n",
            "\t1060\t|\t0.025\t|13.902129173278809\n",
            "\t1070\t|\t0.02\t|-8.534528732299805\n",
            "\t1080\t|\t0.21000000000000005\t|1.5417698621749878\n",
            "\t1090\t|\t0.049999999999999996\t|-25.685558319091797\n",
            "\t1100\t|\t0.13499999999999998\t|2.511652708053589\n",
            "\t1110\t|\t0.38\t|0.6511053442955017\n",
            "\t1120\t|\t0.475\t|-0.11445622146129608\n",
            "\t1130\t|\t0.495\t|-16.44134521484375\n",
            "\t1140\t|\t0.035\t|-5.971092224121094\n",
            "\t1150\t|\t-0.01\t|4.960959434509277\n",
            "\t1160\t|\t0.025000000000000005\t|-22.190942764282227\n",
            "\t1170\t|\t0.0\t|55.406944274902344\n",
            "\t1180\t|\t0.05\t|0.9225894212722778\n",
            "\t1190\t|\t0.08499999999999999\t|-0.2233877331018448\n",
            "\t1200\t|\t0.06999999999999999\t|15.583270072937012\n",
            "\t1210\t|\t0.034999999999999996\t|-35.15068817138672\n",
            "\t1220\t|\t0.12999999999999998\t|0.5305655002593994\n",
            "\t1230\t|\t-0.025\t|-3.0572879314422607\n",
            "\t1240\t|\t0.08\t|-4.806929588317871\n",
            "\t1250\t|\t0.15\t|4.394794464111328\n",
            "\t1260\t|\t0.08\t|2.535261392593384\n",
            "\t1270\t|\t0.04\t|-40.31053161621094\n",
            "\t1280\t|\t0.19\t|1.4786314964294434\n",
            "\t1290\t|\t-0.045\t|-46.397216796875\n",
            "\t1300\t|\t0.15999999999999998\t|55.2948112487793\n",
            "\t1310\t|\t0.075\t|9.058103561401367\n",
            "\t1320\t|\t-0.07000000000000002\t|-181.56204223632812\n",
            "\t1330\t|\t0.23000000000000004\t|9.059404373168945\n",
            "\t1340\t|\t0.02\t|38.68381881713867\n",
            "\t1350\t|\t-0.005\t|3.4150049686431885\n",
            "\t1360\t|\t0.09\t|2.716533660888672\n",
            "\t1370\t|\t-0.025\t|-48.24521255493164\n",
            "\t1380\t|\t0.12499999999999997\t|1.1450228691101074\n",
            "\t1390\t|\t0.23500000000000001\t|1.595272183418274\n",
            "\t1400\t|\t0.030000000000000006\t|1.2993863821029663\n",
            "\t1410\t|\t0.034999999999999996\t|-130.954345703125\n",
            "\t1420\t|\t0.13999999999999999\t|144.3935089111328\n",
            "\t1430\t|\t0.07999999999999999\t|-313.9251403808594\n",
            "\t1440\t|\t0.025\t|2.8369877338409424\n",
            "\t1450\t|\t0.08499999999999999\t|3.2689788341522217\n",
            "\t1460\t|\t-0.07000000000000002\t|-186.79905700683594\n",
            "\t1470\t|\t0.13\t|-7.700564384460449\n",
            "\t1480\t|\t0.18\t|2.6776764392852783\n",
            "\t1490\t|\t0.08\t|-82.06114959716797\n",
            "\t1500\t|\t0.04\t|-2.888763427734375\n",
            "\t1510\t|\t0.12\t|13.658520698547363\n",
            "\t1520\t|\t0.045\t|-210.02667236328125\n",
            "\t1530\t|\t0.1\t|-111.56103515625\n",
            "\t1540\t|\t0.095\t|1.1008180379867554\n",
            "\t1550\t|\t-0.05\t|-257.7124938964844\n",
            "\t1560\t|\t0.12999999999999998\t|3.5652031898498535\n",
            "\t1570\t|\t-0.07500000000000002\t|-161.47291564941406\n",
            "\t1580\t|\t0.11499999999999999\t|-11.815125465393066\n",
            "\t1590\t|\t0.07999999999999999\t|-447.7776184082031\n",
            "\t1600\t|\t0.29500000000000004\t|-98.48991394042969\n",
            "\t1610\t|\t0.1\t|6.226099014282227\n",
            "\t1620\t|\t0.13499999999999998\t|-486.0699157714844\n",
            "\t1630\t|\t0.08\t|-10.4310941696167\n",
            "\t1640\t|\t0.13999999999999999\t|-4.608031749725342\n",
            "\t1650\t|\t0.025000000000000005\t|-128.58010864257812\n",
            "\t1660\t|\t0.025000000000000005\t|69.32852172851562\n",
            "\t1670\t|\t0.045\t|-25.39458465576172\n",
            "\t1680\t|\t0.095\t|2.9346492290496826\n",
            "\t1690\t|\t-0.05500000000000001\t|-798.6647338867188\n",
            "\t1700\t|\t0.1\t|-17.990503311157227\n",
            "\t1710\t|\t-0.045\t|-45.83967208862305\n",
            "\t1720\t|\t-2.7755575615628915e-18\t|15.575393676757812\n",
            "\t1730\t|\t0.039999999999999994\t|3.4016830921173096\n",
            "\t1740\t|\t0.16499999999999998\t|-0.2216419279575348\n",
            "\t1750\t|\t0.15999999999999998\t|1.3527288436889648\n",
            "\t1760\t|\t0.005000000000000003\t|-891.9215698242188\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-70-1af240d5feaf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mepisodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_rewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_training_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeep_agent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"REINFORCE_CNN+LM+NR\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#plotting the results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Number of training episodes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Average return'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-43-3b166559d30f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(deep_agent, env, cfg, tracking, run_name)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_every_N\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m       \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrun_dqn_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeep_agent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_eval_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m       \u001b[0mmean_training_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_training_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-43-3b166559d30f>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_every_N\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m       \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrun_dqn_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeep_agent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_eval_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m       \u001b[0mmean_training_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_training_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-58-5c1824413205>\u001b[0m in \u001b[0;36mrun_dqn_episode\u001b[0;34m(deep_agent, env, eval, max_steps, early_stopping, visit_limit, optimal, render)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#Otherwise we let it act by itself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m       \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeep_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexplore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexplore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m#we keep track of the number of time a celle has been visited to force exploration if visited too frequently, cf higher\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-91c9225b297d>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maction\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m       \u001b[0mnext_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_rows\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0;32melif\u001b[0m \u001b[0maction\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m       \u001b[0mnext_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_rows\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0maction\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/_src/numpy/lax_numpy.py\u001b[0m in \u001b[0;36mdeferring_binary_op\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   5069\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mswap\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5070\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_accepted_binop_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5071\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mbinary_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5072\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_rejected_binop_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5073\u001b[0m       raise TypeError(f\"unsupported operand type(s) for {opchar}: \"\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8gRHBQo6X2K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd9e4ffa-583f-4bc3-bc44-e4e83bc4150b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#########################################\n",
            "#   #     #PP #S#      #         #      #\n",
            "#   #         # #####  #   #     #      #\n",
            "#   ####  #   # #      #   #            #\n",
            "#        G#   # #      #   ###########  #\n",
            "###########         #      #            #\n",
            "#         #P            #  ###          #\n",
            "# P   #   ##################    #########\n",
            "#######   #      #                      #\n",
            "#                       #       P#      #\n",
            "#########################################\n",
            "------\n",
            "#########################################\n",
            "#   #     #PP # #      #         #      #\n",
            "#   #         #S#####  #   #     #      #\n",
            "#   ####  #   # #      #   #            #\n",
            "#        G#   # #      #   ###########  #\n",
            "###########         #      #            #\n",
            "#         #P            #  ###          #\n",
            "# P   #   ##################    #########\n",
            "#######   #      #                      #\n",
            "#                       #       P#      #\n",
            "#########################################\n",
            "------\n",
            "#########################################\n",
            "#   #     #PP # #      #         #      #\n",
            "#   #         # #####  #   #     #      #\n",
            "#   ####  #   #S#      #   #            #\n",
            "#        G#   # #      #   ###########  #\n",
            "###########         #      #            #\n",
            "#         #P            #  ###          #\n",
            "# P   #   ##################    #########\n",
            "#######   #      #                      #\n",
            "#                       #       P#      #\n",
            "#########################################\n",
            "------\n",
            "#########################################\n",
            "#   #     #PP # #      #         #      #\n",
            "#   #         # #####  #   #     #      #\n",
            "#   ####  #   # #      #   #            #\n",
            "#        G#   #S#      #   ###########  #\n",
            "###########         #      #            #\n",
            "#         #P            #  ###          #\n",
            "# P   #   ##################    #########\n",
            "#######   #      #                      #\n",
            "#                       #       P#      #\n",
            "#########################################\n",
            "------\n",
            "#########################################\n",
            "#   #     #PP # #      #         #      #\n",
            "#   #         # #####  #   #     #      #\n",
            "#   ####  #   # #      #   #            #\n",
            "#        G#   # #      #   ###########  #\n",
            "###########    S    #      #            #\n",
            "#         #P            #  ###          #\n",
            "# P   #   ##################    #########\n",
            "#######   #      #                      #\n",
            "#                       #       P#      #\n",
            "#########################################\n",
            "------\n",
            "#########################################\n",
            "#   #     #PP # #      #         #      #\n",
            "#   #         # #####  #   #     #      #\n",
            "#   ####  #   # #      #   #            #\n",
            "#        G#   # #      #   ###########  #\n",
            "###########         #      #            #\n",
            "#         #P   S        #  ###          #\n",
            "# P   #   ##################    #########\n",
            "#######   #      #                      #\n",
            "#                       #       P#      #\n",
            "#########################################\n",
            "------\n",
            "#########################################\n",
            "#   #     #PP # #      #         #      #\n",
            "#   #         # #####  #   #     #      #\n",
            "#   ####  #   # #      #   #            #\n",
            "#        G#   # #      #   ###########  #\n",
            "###########         #      #            #\n",
            "#         #P   S        #  ###          #\n",
            "# P   #   ##################    #########\n",
            "#######   #      #                      #\n",
            "#                       #       P#      #\n",
            "#########################################\n",
            "------\n",
            "#########################################\n",
            "#   #     #PP # #      #         #      #\n",
            "#   #         # #####  #   #     #      #\n",
            "#   ####  #   # #      #   #            #\n",
            "#        G#   # #      #   ###########  #\n",
            "###########         #      #            #\n",
            "#         #P   S        #  ###          #\n",
            "# P   #   ##################    #########\n",
            "#######   #      #                      #\n",
            "#                       #       P#      #\n",
            "#########################################\n",
            "------\n",
            "#########################################\n",
            "#   #     #PP # #      #         #      #\n",
            "#   #         # #####  #   #     #      #\n",
            "#   ####  #   # #      #   #            #\n",
            "#        G#   # #      #   ###########  #\n",
            "###########         #      #            #\n",
            "#         #P   S        #  ###          #\n",
            "# P   #   ##################    #########\n",
            "#######   #      #                      #\n",
            "#                       #       P#      #\n",
            "#########################################\n",
            "------\n",
            "#########################################\n",
            "#   #     #PP # #      #         #      #\n",
            "#   #         # #####  #   #     #      #\n",
            "#   ####  #   # #      #   #            #\n",
            "#        G#   # #      #   ###########  #\n",
            "###########         #      #            #\n",
            "#         #P   S        #  ###          #\n",
            "# P   #   ##################    #########\n",
            "#######   #      #                      #\n",
            "#                       #       P#      #\n",
            "#########################################\n",
            "------\n",
            "#########################################\n",
            "#   #     #PP # #      #         #      #\n",
            "#   #         # #####  #   #     #      #\n",
            "#   ####  #   # #      #   #            #\n",
            "#        G#   # #      #   ###########  #\n",
            "###########         #      #            #\n",
            "#         #P   S        #  ###          #\n",
            "# P   #   ##################    #########\n",
            "#######   #      #                      #\n",
            "#                       #       P#      #\n",
            "#########################################\n",
            "------\n",
            "#########################################\n",
            "#   #     #PP # #      #         #      #\n",
            "#   #         # #####  #   #     #      #\n",
            "#   ####  #   # #      #   #            #\n",
            "#        G#   # #      #   ###########  #\n",
            "###########         #      #            #\n",
            "#         #P   S        #  ###          #\n",
            "# P   #   ##################    #########\n",
            "#######   #      #                      #\n",
            "#                       #       P#      #\n",
            "#########################################\n",
            "------\n",
            "#########################################\n",
            "#   #     #PP # #      #         #      #\n",
            "#   #         # #####  #   #     #      #\n",
            "#   ####  #   # #      #   #            #\n",
            "#        G#   # #      #   ###########  #\n",
            "###########         #      #            #\n",
            "#         #P   S        #  ###          #\n",
            "# P   #   ##################    #########\n",
            "#######   #      #                      #\n",
            "#                       #       P#      #\n",
            "#########################################\n",
            "------\n",
            "#########################################\n",
            "#   #     #PP # #      #         #      #\n",
            "#   #         # #####  #   #     #      #\n",
            "#   ####  #   # #      #   #            #\n",
            "#        G#   # #      #   ###########  #\n",
            "###########         #      #            #\n",
            "#         #P   S        #  ###          #\n",
            "# P   #   ##################    #########\n",
            "#######   #      #                      #\n",
            "#                       #       P#      #\n",
            "#########################################\n",
            "------\n",
            "#########################################\n",
            "#   #     #PP # #      #         #      #\n",
            "#   #         # #####  #   #     #      #\n",
            "#   ####  #   # #      #   #            #\n",
            "#        G#   # #      #   ###########  #\n",
            "###########         #      #            #\n",
            "#         #P   S        #  ###          #\n",
            "# P   #   ##################    #########\n",
            "#######   #      #                      #\n",
            "#                       #       P#      #\n",
            "#########################################\n",
            "-0.1\n"
          ]
        }
      ],
      "source": [
        "# @title **[Run]** Visualize one trajectory for your agent.\n",
        "reward, _ = run_dqn_episode(deep_agent, env, eval=True, max_steps = 100, optimal = False, render=True)\n",
        "print(reward)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Auxiliary function to plot the policy learned"
      ],
      "metadata": {
        "id": "8emEwwjztmI9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The direction of the arrow correspond to the optimal action ie \">\" stands for \"right\", \"<\" for \"left\", \"^\" for \"up\" and \"v\" for \"down\"."
      ],
      "metadata": {
        "id": "6PuuGqYntrv7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "network = deep_agent.apply\n",
        "op = deep_agent._best_params\n",
        "m, n, _= env.obs_shape()\n",
        "\n",
        "env = Maze(BIG_MAZE)\n",
        "grid = env.reset()\n",
        "print(env.render())\n",
        "action_map = [[None for y in range(n)] for x in range(m)]\n",
        "xp, yp = np.nonzero(grid[..., CellType.PLAYER.value - 1])\n",
        "xp, yp = xp[0], yp[0]\n",
        "grid[xp, yp, CellType.PLAYER.value - 1] = False\n",
        "\n",
        "action_to_char = {\n",
        "    0: \"v\",\n",
        "    1: \"^\",\n",
        "    2: \">\",\n",
        "    3: \"<\"\n",
        "}\n",
        "for x in range(m):\n",
        "  for y in range(n):\n",
        "    state = grid.copy()\n",
        "    if grid[..., CellType.WALL.value - 1][x, y]:\n",
        "      action_map[x][y] = \"#\"\n",
        "      continue\n",
        "    if grid[..., CellType.PIT.value - 1][x, y]:\n",
        "      action_map[x][y] = \"P\"\n",
        "      continue\n",
        "    if grid[..., CellType.GOAL.value - 1][x, y]:\n",
        "      action_map[x][y] = \"G\"\n",
        "      continue\n",
        "    print(op)\n",
        "    #print(state[None])\n",
        "    state[x, y, CellType.PLAYER.value - 1] = True\n",
        "    action = np.argmax(network(op, state[None]), axis=-1)\n",
        "    action_map[x][y] = action_to_char[action]\n",
        "\n",
        "\n",
        "print(\"--------------------------------------\")\n",
        "policy = \"\\n\".join([\"\".join([action_map[x][y] for y in range(n)]) for x in range(m)])\n",
        "print(policy)"
      ],
      "metadata": {
        "id": "7cERz2B1Xvmf"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "sQQDt7TKI4Ru",
        "6wWae--BHrgs",
        "qgkq1IKnIO6M",
        "GvuwWP3PI-PY",
        "qWtjeC51IxcW",
        "8emEwwjztmI9"
      ]
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}