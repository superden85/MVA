{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we write our custom matricization function\n",
    "def matricization(A, mode):\n",
    "\n",
    "    if mode == 1:\n",
    "        return torch.cat([A.select(2, i) for i in range(A.size(2))], dim=1)\n",
    "    \n",
    "    elif mode == 2:\n",
    "        return torch.cat([A.select(2, i).T for i in range(A.size(2))], dim=1)\n",
    "    \n",
    "    elif mode == 3:\n",
    "        return torch.vstack([A.select(2, i).T.reshape(-1) for i in range(A.size(2))])\n",
    "\n",
    "\n",
    "    \"\"\" p1, p2, p3 = A.shape\n",
    "    if mode == 1:\n",
    "        res = torch.zeros(p1, p2 * p3)\n",
    "        for i in range(p1):\n",
    "            for j in range(p2):\n",
    "                for k in range(p3):\n",
    "                    res[i, k * p2 + j] = A[i, j, k]\n",
    "    elif mode == 2:\n",
    "        res = torch.zeros(p2, p1 * p3)\n",
    "        for i in range(p1):\n",
    "            for j in range(p2):\n",
    "                for k in range(p3):\n",
    "                    res[j, k * p1 + i] = A[i, j, k]\n",
    "    elif mode == 3:\n",
    "        res = torch.zeros(p3, p1 * p2)\n",
    "        for i in range(p1):\n",
    "            for j in range(p2):\n",
    "                for k in range(p3):\n",
    "                    res[k, j * p1 + i] = A[i, j, k]\n",
    "    return res \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1., 13.],\n",
      "         [ 4., 16.],\n",
      "         [ 7., 19.],\n",
      "         [10., 22.]],\n",
      "\n",
      "        [[ 2., 14.],\n",
      "         [ 5., 17.],\n",
      "         [ 8., 20.],\n",
      "         [11., 23.]],\n",
      "\n",
      "        [[ 3., 15.],\n",
      "         [ 6., 18.],\n",
      "         [ 9., 21.],\n",
      "         [12., 24.]]])\n",
      "Matricization along mode-1:\n",
      " tensor([[ 1.,  4.,  7., 10., 13., 16., 19., 22.],\n",
      "        [ 2.,  5.,  8., 11., 14., 17., 20., 23.],\n",
      "        [ 3.,  6.,  9., 12., 15., 18., 21., 24.]])\n",
      "Matricization along mode-2:\n",
      " tensor([[ 1.,  2.,  3., 13., 14., 15.],\n",
      "        [ 4.,  5.,  6., 16., 17., 18.],\n",
      "        [ 7.,  8.,  9., 19., 20., 21.],\n",
      "        [10., 11., 12., 22., 23., 24.]])\n",
      "Matricization along mode-3:\n",
      " tensor([[ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12.],\n",
      "        [13., 14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24.]])\n"
     ]
    }
   ],
   "source": [
    "#test matricization with the examples in the Kolda and Bader paper\n",
    "A = torch.zeros(3, 4, 2)\n",
    "A[:, :, 0] = torch.arange(1, 13).reshape(4, 3).transpose(0, 1)\n",
    "A[:, :, 1] = torch.arange(13, 25).reshape(4, 3).transpose(0, 1)\n",
    "\n",
    "print(A)\n",
    "A1 = matricization(A, 1)\n",
    "A2 = matricization(A, 2)\n",
    "A3 = matricization(A, 3)\n",
    "\n",
    "print('Matricization along mode-1:\\n', A1)\n",
    "print('Matricization along mode-2:\\n', A2)\n",
    "print('Matricization along mode-3:\\n', A3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# we also write the inverse operation, only from A(1) to A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inv_matricization(A, p1, p2, p3):\n",
    "    res = torch.zeros(p1, p2, p3)\n",
    "    for i in range(p1):\n",
    "        for j in range(p2):\n",
    "            for k in range(p3):\n",
    "                res[i, j, k] = A[i, k * p2 + j]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_A(N, P, diagonal, r1, r2, r3):\n",
    "    G = torch.zeros(r1, r2, r3)\n",
    "    G[0, 0, 0] = diagonal[0]\n",
    "    G[1, 1, 1] = diagonal[1]\n",
    "    G[2, 2, 2] = diagonal[2]\n",
    "\n",
    "    #we generate matrices until we have a stable model, according to assumption 1 in the paper\n",
    "    while True:\n",
    "        O1 = torch.randn(N, r1)\n",
    "        O2 = torch.randn(N, r2)\n",
    "        O3 = torch.randn(P, r3)\n",
    "\n",
    "        #generate Ui as the top ri singular vectors of the matrix\n",
    "        U1 = torch.linalg.svd(O1)[0][:, :r1]\n",
    "        U2 = torch.linalg.svd(O2)[0][:, :r2]\n",
    "        U3 = torch.linalg.svd(O3)[0][:, :r3]\n",
    "        \n",
    "\n",
    "        #build A, given its tucker decomposition above\n",
    "        #mode 1 product of U1 and G\n",
    "        A1 = torch.einsum('ij, jkl -> ikl', U1, G)\n",
    "        #print(A1.shape)\n",
    "        #mode 2 product of A1 and U2\n",
    "        A2 = torch.einsum('ij, kjl -> kil', U2, A1)\n",
    "        #print(A2.shape)\n",
    "        #mode 3 product of A2 and U3\n",
    "        A = torch.einsum('ij, klj -> kli', U3, A2)\n",
    "        #print(A.shape)\n",
    "\n",
    "        \"\"\" A1 = torch.tensordot(U1, G, dims=[[1], [0]])\n",
    "        #print(A1.shape)\n",
    "        A2 = torch.tensordot(U2, A1, dims=[[1], [1]])\n",
    "        #print(A2.shape)\n",
    "        A = torch.tensordot(A2, U3, dims=[[2], [1]])\n",
    "        #print(A.shape) \"\"\"\n",
    "\n",
    "        #check if the model is stable by \n",
    "        # Computing the companion matrix\n",
    "        C = torch.zeros(N*P, N*P)\n",
    "        for i in range(P):\n",
    "            C[:N, i*N:(i+1)*N] = A[:, :, i]\n",
    "        for i in range(P-1):\n",
    "            C[(i+1)*N:(i+2)*N, i*N:(i+1)*N] = torch.eye(N)\n",
    "        \n",
    "        # Compute the eigenvalues of the companion matrix\n",
    "        eigenvalues = torch.linalg.eigvals(C)\n",
    "        # Check if all eigenvalues are inside the unit circle\n",
    "        if torch.all(torch.abs(eigenvalues) < 1):\n",
    "            #print('The model is stable')\n",
    "            return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def var_generate(A, T=100):\n",
    "\n",
    "    #A is a (N, N, P) tensor\n",
    "    N, P = A.shape[0], A.shape[2]\n",
    "\n",
    "    #let's matricize A to use the model in (3)\n",
    "    Ac = matricization(A, 1)\n",
    "    \n",
    "    #we return x, a (T, NP) tensor\n",
    "    x = torch.zeros(T, N*P)\n",
    "    x[0, :] = torch.randn(N*P)\n",
    "\n",
    "    #we also return y, a (T, N) tensor\n",
    "    y = torch.zeros(T, N)\n",
    "    for t in range(T):\n",
    "        y[t, :] = Ac @ x[t] + torch.randn(N)\n",
    "        if t < T-1:\n",
    "            x[t+1, :N] = y[t, :]\n",
    "            x[t+1, N:] = x[t, :-N]\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data generation\n",
    "N, P = 10, 5\n",
    "r1, r2, r3 = 3, 3, 3\n",
    "diagonal_list = [(2, 2, 2), (4, 3, 2), (1, 1, 1), (2, 1, 0.5)]\n",
    "\n",
    "#let's construct the transitions matrices as built in the paper\n",
    "A_list = []\n",
    "for diagonal in diagonal_list:\n",
    "    A_list.append(generate_A(N, P, diagonal, r1, r2, r3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_estimator(x, y, lambd=0.1, max_iter=100):\n",
    "    T,N = y.shape\n",
    "    P = x.shape[1]//N\n",
    "    #define the loss function as in the paper\n",
    "    def loss(A):\n",
    "        #A is of shape (N, NP)\n",
    "        return torch.sum((y.T - A @ x.T)**2) + lambd * torch.norm(A, p='nuc')\n",
    "    \n",
    "    #we use the gradient descent algorithm to minimize the loss function\n",
    "    A = torch.randn(N, N*P, requires_grad=True)\n",
    "    optimizer = torch.optim.Adam([A], lr=0.1)\n",
    "\n",
    "    #We use SGD to compute the argmin of the loss function\n",
    "    for i in range(max_iter):\n",
    "        l = loss(A)\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    return A.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Frobenius norm of the error between the initial estimator and the true transition matrix is:\n",
      " tensor(3.4645)\n"
     ]
    }
   ],
   "source": [
    "#test the initial estimator on the first transition matrix\n",
    "x, y = var_generate(A_list[0], T=100)\n",
    "A = initial_estimator(x, y, lambd=1, max_iter=1000)\n",
    "\n",
    "#print the Frobenius norm of the error between the initial estimator and the true transition matrix\n",
    "print('The Frobenius norm of the error between the initial estimator and the true transition matrix is:\\n', torch.norm(A - matricization(A_list[0], 1)))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rank selection consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multilinear_ranks_estimator(A, c):\n",
    "    ranks = [0, 0, 0]\n",
    "    for mode in range(3):\n",
    "        try:\n",
    "            eigvals = torch.linalg.svdvals(matricization(A, mode+1))\n",
    "            ranks[mode] = torch.argmin((eigvals[1:]+c)/(eigvals[:-1]+c)).item() + 1\n",
    "        except torch.linalg.LinAlgError as e:\n",
    "            ranks[mode] = 3\n",
    "\n",
    "    return ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Denis\\OneDrive\\Documents\\cours_mva\\series_temporelles\\mini_projet\\mini_project.ipynb Cell 16\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Denis/OneDrive/Documents/cours_mva/series_temporelles/mini_projet/mini_project.ipynb#X21sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m x, y \u001b[39m=\u001b[39m var_generate(A, T\u001b[39m=\u001b[39mT)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Denis/OneDrive/Documents/cours_mva/series_temporelles/mini_projet/mini_project.ipynb#X21sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m#we estimate the transition matrix\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Denis/OneDrive/Documents/cours_mva/series_temporelles/mini_projet/mini_project.ipynb#X21sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m A_hat \u001b[39m=\u001b[39m initial_estimator(x, y, lambd\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, max_iter\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Denis/OneDrive/Documents/cours_mva/series_temporelles/mini_projet/mini_project.ipynb#X21sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m tensor \u001b[39m=\u001b[39m inv_matricization(A_hat, N, N, P)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Denis/OneDrive/Documents/cours_mva/series_temporelles/mini_projet/mini_project.ipynb#X21sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m#we estimate the multilinear ranks\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Denis/OneDrive/Documents/cours_mva/series_temporelles/mini_projet/mini_project.ipynb#X21sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m#ranks = multilinear_ranks_estimator(A, c)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Denis/OneDrive/Documents/cours_mva/series_temporelles/mini_projet/mini_project.ipynb#X21sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m#we check if the estimator is correct\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\Denis\\OneDrive\\Documents\\cours_mva\\series_temporelles\\mini_projet\\mini_project.ipynb Cell 16\u001b[0m in \u001b[0;36minitial_estimator\u001b[1;34m(x, y, lambd, max_iter)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Denis/OneDrive/Documents/cours_mva/series_temporelles/mini_projet/mini_project.ipynb#X21sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(max_iter):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Denis/OneDrive/Documents/cours_mva/series_temporelles/mini_projet/mini_project.ipynb#X21sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     l \u001b[39m=\u001b[39m loss(A)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Denis/OneDrive/Documents/cours_mva/series_temporelles/mini_projet/mini_project.ipynb#X21sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     l\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Denis/OneDrive/Documents/cours_mva/series_temporelles/mini_projet/mini_project.ipynb#X21sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Denis/OneDrive/Documents/cours_mva/series_temporelles/mini_projet/mini_project.ipynb#X21sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\Denis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    355\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    356\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    357\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    361\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    362\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> 363\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32mc:\\Users\\Denis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:166\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    162\u001b[0m inputs \u001b[39m=\u001b[39m (inputs,) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(inputs, torch\u001b[39m.\u001b[39mTensor) \u001b[39melse\u001b[39;00m \\\n\u001b[0;32m    163\u001b[0m     \u001b[39mtuple\u001b[39m(inputs) \u001b[39mif\u001b[39;00m inputs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mtuple\u001b[39m()\n\u001b[0;32m    165\u001b[0m grad_tensors_ \u001b[39m=\u001b[39m _tensor_or_tensors_to_tuple(grad_tensors, \u001b[39mlen\u001b[39m(tensors))\n\u001b[1;32m--> 166\u001b[0m grad_tensors_ \u001b[39m=\u001b[39m _make_grads(tensors, grad_tensors_, is_grads_batched\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    167\u001b[0m \u001b[39mif\u001b[39;00m retain_graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n",
      "File \u001b[1;32mc:\\Users\\Denis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:68\u001b[0m, in \u001b[0;36m_make_grads\u001b[1;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[39mif\u001b[39;00m out\u001b[39m.\u001b[39mnumel() \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m     67\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mgrad can be implicitly created only for scalar outputs\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 68\u001b[0m     new_grads\u001b[39m.\u001b[39mappend(torch\u001b[39m.\u001b[39;49mones_like(out, memory_format\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mpreserve_format))\n\u001b[0;32m     69\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     70\u001b[0m     new_grads\u001b[39m.\u001b[39mappend(\u001b[39mNone\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#we want to estimate the consistency of the multilinear ranks estimator\n",
    "#we will use the ridge-type ratio estimator, as in the paper\n",
    "#Theorem 3 in the paper says that the estimator is consistent\n",
    "#so as T increases, the probability of being correct tends to 1\n",
    "\n",
    "n_replications = 10\n",
    "\n",
    "T_list = np.arange(50, 60)\n",
    "proportions = np.zeros((4, T_list.shape[0]))\n",
    "\n",
    "\n",
    "for i, T in enumerate(T_list):\n",
    "    c = np.sqrt(N*P*np.log(T)/(10*T))\n",
    "    for j, A in enumerate(A_list):\n",
    "        for n in range(n_replications):\n",
    "            #we generate the data\n",
    "            x, y = var_generate(A, T=T)\n",
    "            #we estimate the transition matrix\n",
    "            A_hat = initial_estimator(x, y, lambd=1, max_iter=1000)\n",
    "            \n",
    "            tensor = inv_matricization(A_hat, N, N, P)\n",
    "            #we estimate the multilinear ranks\n",
    "            ranks = multilinear_ranks_estimator(A, c)\n",
    "            #we check if the estimator is correct\n",
    "            if ranks == [3, 3, 3]:\n",
    "                proportions[j, i] += 1/n_replications\n",
    "\n",
    "\n",
    "#plot the proportion of correct estimations\n",
    "plt.plot(T_list, proportions[0, :], label='(2, 2, 2)')\n",
    "plt.plot(T_list, proportions[1, :], label='(4, 3, 2)')\n",
    "plt.plot(T_list, proportions[2, :], label='(1, 1, 1)')\n",
    "plt.plot(T_list, proportions[3, :], label='(2, 1, 0.5)')\n",
    "plt.legend()\n",
    "plt.xlabel('T')\n",
    "plt.ylabel('Proportion of correct estimations')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm for the estimator A_MLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alternating_squares_MLR(y,x,X,A_0,ranks,max_iter=100):\n",
    "\n",
    "    T = y.shape[0]\n",
    "    N = y.shape[1]\n",
    "    P = x.shape[1] // N\n",
    "    A = A_0 #shape (N, N, P)\n",
    "\n",
    "    #get the ranks\n",
    "    r1, r2, r3 = ranks[0], ranks[1], ranks[2]\n",
    "    print(r1, r2, r3)\n",
    "\n",
    "    #compute the HOSVD of A\n",
    "\n",
    "    A1 = matricization(A, 1)\n",
    "    A2 = matricization(A, 2)\n",
    "    A3 = matricization(A, 3)\n",
    "    U1, S1, V1 = torch.linalg.svd(A1)\n",
    "    U2, S2, V2 = torch.linalg.svd(A2)\n",
    "    U3, S3, V3 = torch.linalg.svd(A3)\n",
    "\n",
    "    #compute the core tensor\n",
    "    G = torch.zeros(r1, r2, r3)\n",
    "    for i in range(r1):\n",
    "        for j in range(r2):\n",
    "            for k in range(r3):\n",
    "                G[i, j, k] = S1[i] * S2[j] * S3[k]\n",
    "\n",
    "    #compute the matricization of G\n",
    "    G1 = matricization(G, 1)\n",
    "\n",
    "    #compute the factor matrices\n",
    "    U1 = U1[:, :r1]\n",
    "    U2 = U2[:, :r2]\n",
    "    U3 = U3[:, :r3]\n",
    "\n",
    "    \n",
    "    for k in range(max_iter):\n",
    "\n",
    "        #update U1\n",
    "        Y = torch.zeros(N*r1)\n",
    "        A = torch.zeros(N*r1, N*r1)\n",
    "        H = torch.kron(U3.contiguous(), U2.contiguous()) @ G1.T\n",
    "        for t in range(T):\n",
    "            xt = x[t]\n",
    "            At = torch.kron(xt.T @ H , torch.eye(N))\n",
    "            Y += At.T @ y[t]\n",
    "            A += At.T @ At\n",
    "        if torch.linalg.det(A) == 0:\n",
    "            A += 1e-2 * torch.eye(N*r1)\n",
    "        U1 = torch.linalg.solve(A, Y).reshape(N, r1)\n",
    "\n",
    "        #update U2\n",
    "        Y = torch.zeros(N*r2)\n",
    "        A = torch.zeros(N*r2, N*r2)\n",
    "        H = U1 @ G1\n",
    "        for t in range(T):\n",
    "            Xt = X[t]\n",
    "            At = H @ torch.kron((Xt @ U3).T.contiguous(), torch.eye(r2))\n",
    "            Y += At.T @ y[t]\n",
    "            A += At.T @ At\n",
    "        if torch.linalg.det(A) == 0:\n",
    "            A += 1e-2 * torch.eye(N*r2)\n",
    "        U2 = torch.linalg.solve(A, Y).reshape(r2, N).T\n",
    "\n",
    "        #update U3\n",
    "        Y = torch.zeros(P*r3)\n",
    "        A = torch.zeros(P*r3, P*r3)\n",
    "        H = U1 @ G1\n",
    "        for t in range(T):\n",
    "            Xt = X[t]\n",
    "            At = H @ torch.kron(torch.eye(r3), (U2.T @ Xt))\n",
    "            Y += At.T @ y[t]\n",
    "            A += At.T @ At\n",
    "        if torch.linalg.det(A) == 0:\n",
    "            A += 1e-2 * torch.eye(P*r3)\n",
    "        U3 = torch.linalg.solve(A, Y).reshape(P, r3)\n",
    "\n",
    "        #update G1\n",
    "        Y = torch.zeros(r1*r2*r3)\n",
    "        A = torch.zeros(r1*r2*r3, r1*r2*r3)\n",
    "        for t in range(T):\n",
    "            xt = x[t]\n",
    "            At = torch.kron((torch.kron(U3.contiguous(), U2.contiguous()).T @ xt).T.contiguous(), U1.contiguous())\n",
    "            Y += At.T @ y[t]\n",
    "            A += At.T @ At\n",
    "        if torch.linalg.det(A) == 0:\n",
    "            A += 1e-2 * torch.eye(r1*r2*r3)\n",
    "        G1 = torch.linalg.solve(A, Y).reshape(r1, r2 * r3)\n",
    "\n",
    "    #return A, whose tucker decomposition is U1, U2, U3, G\n",
    "    G = inv_matricization(G1, r1, r2, r3)\n",
    "\n",
    "    #print(G.shape, G1.shape, U1.shape, U2.shape, U3.shape)\n",
    "    #mode 1 product of U1 and G\n",
    "    A1 = torch.einsum('ij, jkl -> ikl', U1, G)\n",
    "    #print(A1.shape)\n",
    "    #mode 2 product of A1 and U2\n",
    "    A2 = torch.einsum('ij, kjl -> kil', U2, A1)\n",
    "    #print(A2.shape)\n",
    "    #mode 3 product of A2 and U3\n",
    "    A = torch.einsum('ij, klj -> kli', U3, A2)\n",
    "    #print(A.shape)\n",
    "    \n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 3 3\n",
      "tensor(3.7943)\n"
     ]
    }
   ],
   "source": [
    "#trying the alternating squares algorithm\n",
    "#we will use the same data as before\n",
    "T = 100\n",
    "x, y = var_generate(A_list[0], T)\n",
    "X = x.reshape(T, N, P)\n",
    "\n",
    "A_0 = initial_estimator(x, y, lambd=1, max_iter=1000)\n",
    "A_0 = inv_matricization(A_0, N, N, P)\n",
    "c = np.sqrt(N*P*np.log(T)/(10*T))\n",
    "ranks = multilinear_ranks_estimator(A_0, c)\n",
    "A = alternating_squares_MLR(y, x, X, A_0, ranks, max_iter=20)\n",
    "\n",
    "#print the Frobenius norm of the difference between A and true_A\n",
    "true_A = A_list[0]\n",
    "#print(A)\n",
    "print(torch.norm(A - true_A))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def admm_subroutine(y, X, B, N, r, T, kappa=1, lambd=1, max_iter1=100, max_iter2=100, sgd_steps=10):\n",
    "    #y is a NT x 1 vector\n",
    "    #X is a NT x NPT matrix\n",
    "    \n",
    "    #initialization\n",
    "\n",
    "    W = B.clone()\n",
    "    M = torch.zeros(N * r)\n",
    "    for k in range(max_iter1):\n",
    "\n",
    "        #update B\n",
    "        #using the SOC method, cited in the paper\n",
    "        I = B.clone()\n",
    "        J = B.clone()\n",
    "        K = B.clone()\n",
    "        def loss(I):\n",
    "                return (1/T)*torch.norm(y - X @ I)**2 + kappa * torch.norm(I-W+M)**2 + 0.5*torch.norm(I-J+K)**2\n",
    "\n",
    "        for j in range(max_iter2):\n",
    "\n",
    "            #update I\n",
    "            #we do gradient descent for this one\n",
    "            #Adam optimizer\n",
    "            optimizer = torch.optim.Adam([I], lr=0.01)\n",
    "            for _ in range(sgd_steps):\n",
    "                optimizer.zero_grad()\n",
    "                loss(I).backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            #update J\n",
    "            Y = I + K\n",
    "            U, D, V = torch.linalg.svd(Y)\n",
    "            J = U @ torch.eye(N, r) @ V.T\n",
    "\n",
    "            #update K\n",
    "            K = K + I - J\n",
    "        B = I\n",
    "\n",
    "\n",
    "        #update W\n",
    "        #we have to solve a least squares problem with a l1 penalty\n",
    "        #we use ista\n",
    "\n",
    "        def soft_threshold(x, lambd):\n",
    "            return torch.sign(x) * torch.clamp(torch.abs(x) - lambd, min=0)\n",
    "\n",
    "        def ista(b, lambd, max_iter=100):\n",
    "            x = torch.zeros(b.shape)\n",
    "            L = b.shape[0]\n",
    "            for i in range(max_iter):\n",
    "                x = soft_threshold(x + (b - x) / L, lmbda / L)\n",
    "            return x\n",
    "\n",
    "        W = ista(B+M, lambd/kappa, max_iter=100)\n",
    "\n",
    "\n",
    "        #update M\n",
    "        M = M + B - W\n",
    "\n",
    "    return B"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm for the estimator A_SHORR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ADMM_SHORR(y,x,X,A_0,ranks,max_iter=100, lambd=1, rho1=1, rho2=1, rho3=1):\n",
    "\n",
    "    T = y.shape[0]\n",
    "    N = y.shape[1]\n",
    "    P = x.shape[1] // N\n",
    "    A = A_0 #shape (N, N, P)\n",
    "\n",
    "    #get the ranks\n",
    "    r1, r2, r3 = ranks[0], ranks[1], ranks[2]\n",
    "    print(r1, r2, r3)\n",
    "\n",
    "    #compute the HOSVD of A\n",
    "\n",
    "    A1 = matricization(A, 1)\n",
    "    A2 = matricization(A, 2)\n",
    "    A3 = matricization(A, 3)\n",
    "    U1, S1, V1 = torch.linalg.svd(A1)\n",
    "    U2, S2, V2 = torch.linalg.svd(A2)\n",
    "    U3, S3, V3 = torch.linalg.svd(A3)\n",
    "\n",
    "    #compute the core tensor\n",
    "    G = torch.zeros(r1, r2, r3)\n",
    "    for i in range(r1):\n",
    "        for j in range(r2):\n",
    "            for k in range(r3):\n",
    "                G[i, j, k] = S1[i] * S2[j] * S3[k]\n",
    "\n",
    "    #compute the matricizations of G\n",
    "    G_mats = [matricization(G, 1), matricization(G, 2), matricization(G, 3)]\n",
    "\n",
    "\n",
    "    #compute the factor matrices\n",
    "    U1 = U1[:, :r1]\n",
    "    U2 = U2[:, :r2]\n",
    "    U3 = U3[:, :r3]\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    for k in range(max_iter):\n",
    "\n",
    "        #update U1\n",
    "\n",
    "        H = torch.kron(U3.contiguous(), U2.contiguous()) @ G_mats[0].T\n",
    "        X1 = torch.zeros(N*T, N*r1)\n",
    "        for t in range(T):\n",
    "            xt = x[t]\n",
    "            At = torch.kron(xt.T @ H , torch.eye(N))\n",
    "            X1[t*N:(t+1)*N, :] = At\n",
    "        U1 = admm_subroutine(y.reshape(-1), X1, U1.T.reshape(-1), N, r1, T, kappa=1, lambd=lambd*torch.norm(U2, ord=1)*torch.norm(U3, ord=1), max_iter1=100, max_iter2=100, sgd_steps=10)\n",
    "\n",
    "        #update U2\n",
    "\n",
    "        H = U1 @ G_mats[0]\n",
    "        X2 = torch.zeros(N*T, N*r2)\n",
    "        for t in range(T):\n",
    "            Xt = X[t]\n",
    "            At = H @ torch.kron((Xt @ U3).T.contiguous(), torch.eye(r2))\n",
    "            X2[t*N:(t+1)*N, :] = At\n",
    "        U2 = admm_subroutine(y.reshape(-1), X2, U2.T.reshape(-1), N, r2, T, kappa=1, lambd=lambd*torch.norm(U1, ord=1)*torch.norm(U3, ord=1), max_iter1=100, max_iter2=100, sgd_steps=10)\n",
    "\n",
    "        #update U3\n",
    "        H = U1 @ G_mats[0]\n",
    "        X3 = torch.zeros(N*T, N*r3)\n",
    "        for t in range(T):\n",
    "            Xt = X[t]\n",
    "            At = H @ torch.kron(torch.eye(r3), (U2.T @ Xt))\n",
    "            X3[t*N:(t+1)*N, :] = At\n",
    "        U3 = admm_subroutine(y.reshape(-1), X3, U3.T.reshape(-1), N, r3, T, kappa=1, lambd=lambd*torch.norm(U1, ord=1)*torch.norm(U2, ord=1), max_iter1=100, max_iter2=100, sgd_steps=10)\n",
    "\n",
    "\n",
    "        #update G\n",
    "\n",
    "        #we will use SGD on a square loss\n",
    "\n",
    "        def loss(G):\n",
    "            return torch.sum((y - X @ (U1 @ G @ U2.T @ U3.T))**2) \n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "    #return A, whose tucker decomposition is U1, U2, U3, G\n",
    "    G = inv_matricization(G1, r1, r2, r3)\n",
    "\n",
    "    #print(G.shape, G1.shape, U1.shape, U2.shape, U3.shape)\n",
    "    #mode 1 product of U1 and G\n",
    "    A1 = torch.einsum('ij, jkl -> ikl', U1, G)\n",
    "    #print(A1.shape)\n",
    "    #mode 2 product of A1 and U2\n",
    "    A2 = torch.einsum('ij, kjl -> kil', U2, A1)\n",
    "    #print(A2.shape)\n",
    "    #mode 3 product of A2 and U3\n",
    "    A = torch.einsum('ij, klj -> kli', U3, A2)\n",
    "    #print(A.shape)\n",
    "    \n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_mlr1 = matricization(A_mlr, 1)\n",
    "A_ols1 = matricization(A_ols, 1)\n",
    "A_rrr1 = matricization(A_rrr, 1)\n",
    "\n",
    "mlr_preds = x @ A_mlr1\n",
    "ols_preds = x @ A_ols1\n",
    "rrr_preds = x @ A_rrr1\n",
    "\n",
    "#plot the results on the same plot with the true values of y\n",
    "#on a big plot\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "plt.plot(y[:, -3], label='True')\n",
    "plt.plot(mlr_preds[:, -3], label='MLR')\n",
    "plt.plot(ols_preds[:, -3], label='OLS')\n",
    "plt.plot(rrr_preds[:, -3], label='RRR')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
